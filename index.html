---
# You don't need to edit this file, it's empty on purpose.
# Edit theme's home layout instead if you wanna make some changes
# See: https://jekyllrb.com/docs/themes/#overriding-theme-defaults
layout: home
author_profile: true
---


<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.1//EN"
  "http://www.w3.org/TR/xhtml11/DTD/xhtml11.dtd">
<html xmlns="http://www.w3.org/1999/xhtml" xml:lang="en">
<head>
<meta name="generator" content="jemdoc, see http://jemdoc.jaboc.net/" />
<meta http-equiv="Content-Type" content="text/html;charset=utf-8" />
<link rel="stylesheet" href="jemdoc.css" type="text/css" />
<title>Jia-Bin Huang</title>
</head>
<body>
<script type="text/javascript">
var gaJsHost = (("https:" == document.location.protocol) ? "https://ssl." : "http://www.");
document.write(unescape("%3Cscript src='" + gaJsHost + "google-analytics.com/ga.js' type='text/javascript'%3E%3C/script%3E"));
</script>
<script type="text/javascript">
try {
    var pageTracker = _gat._getTracker("UA-15688631-4");
    pageTracker._trackPageview();
} catch(err) {}</script>
<div id="layout-content">
<div id="toptitle">
<h1>Jia-Bin Huang</h1>
</div>
<table class="imgtable"><tr><td>
<img src="images/jbhuang.jpg" alt="Jia-Bin Huang" width="200px" />&nbsp;</td>
<td align="left"><p><b>Capital One endowed Associate Professor</b> <br />
<a href="https://www.cs.umd.edu/">Department of Computer Science</a>, <a href="https://www.umd.edu/">University of Maryland, College Park</a></p>
<p><b>Research Scientist</b> <br />
<a href="https://about.facebook.com/realitylabs/">Reality Labs, Meta (FRL)</a></p>
<p><b>Adjunct Assistant Professor</b> (2022 - 2024) <br />
<a href="https://cs.unc.edu/">Department of Computer Science</a>, <a href="https://www.unc.edu/">University of North Carolina at Chapel Hill</a></p>
<p>I did my PhD in the <a href="https://ece.illinois.edu/">ECE department</a> at the <a href="https://illinois.edu/">University of Illinois, Urbana-Champaign</a> advised by <a href="http://vision.ai.illinois.edu/ahuja.html">Narendra Ahuja</a>.
Over the summers, I am lucky to have opportunities to work with
<a href="http://johanneskopf.de/">Johannes Kopf</a>,
<a href="http://szeliski.org/RichardSzeliski.htm">Richard Szeliski</a>,
<a href="http://www.singbingkang.com/">Sing Bing Kang</a>,
<a href="https://en.wikipedia.org/wiki/Zhengyou_Zhang">Zhengyou Zhang</a>,
<a href="https://www.microsoft.com/en-us/research/people/rcaruana/">Rich Caruana</a>,
<a href="https://www.cs.ubc.ca/~lsigal/">Leonid Sigal</a>, and
<a href="https://faculty.ucmerced.edu/mhyang/">Ming-Hsuan Yang</a>.
Prior to my graduate study, I worked with <a href="https://imp.iis.sinica.edu.tw/">Chu-Song Chen</a> at <a href="https://www.iis.sinica.edu.tw/en/index.html">IIS, Academia Sinica</a>.
I received my B.S from <a href="https://www.nctu.edu.tw/en">National Chiao-Tung University</a>, working with <a href="https://eenctu.nctu.edu.tw/en/teacher/p1.php?num=94&amp;page=1">Sheng-Jyh Wang</a>.</p>
<p>My research interests lie at the intersection of computer vision, computer graphics, and machine learning.</p>
<h3><a href="#group">Research Group</a> | <a href="#teaching">Teaching</a> | <a href="#talks">Talks</a> | <a href="#pubs">Publications</a> | <a href="Huang_CV.pdf">CV</a> | <a href="Huang_CV_Failure.pdf">CV of Failures</a> | <a href="#open-office-hour">Open Office Hours</a> | <a href="https://github.com/jbhuang0604/awesome-tips">Twitter tips</a></h3>
<p>
<a href="https://twitter.com/jbhuang0604">
<a class="twitter-follow-button"
href="https://twitter.com/jbhuang0604"
data-size="large">
Follow @jbhuang0604</a>
 <br />
<b>Email</b>: <a href="mailto:jbhuang@vt.edu">jbhuang@vt.edu</a> | <a href="mailto:jbhuang@fb.com">jbhuang@fb.com</a> | <a href="mailto:jbhuang1@cs.unc.edu">jbhuang1@cs.unc.edu</a></p>
</td></tr></table>
<div class="infoblock">
<div class="blockcontent">
<ul>
<li><p><b>Prospective MS/PhD students</b>:
We will be recruiting highly motivated students at all levels to join our lab starting Fall 2023.
Please <a href="https://www.cs.umd.edu/grad/catalog">apply through the CS department</a> and include my name as a possible advisor in your application.
Have questions? Check out <a href="advisor_guide.html">my answers to the PhD Advisor Guide</a>.</p>
</li>
<li><p><b>Prospective Visiting Students</b>: For short-term visiting (e.g., 6 months or more), please fill in the form <a href="https://forms.gle/71Ac4XPPoEw1W45P8">here</a>.</p>
</li>
<li><p><b>Prospective Faculty at UMD</b>: The CS department is hiring multiple faculty! Find out more <a href="https://www.cs.umd.edu/about/employment/all">here</a>.



</p>
</li>
</ul>
</div></div>
<a class="twitter-timeline" data-width="800" data-height="600" href="https://twitter.com/jbhuang0604?ref_src=twsrc%5Etfw">Tweets by jbhuang0604</a> <script async src="https://platform.twitter.com/widgets.js" charset="utf-8"></script>
<p>
<a name="talks"></a>
</p>
<h2>Recent talks</h2>
<table id="Highlights">
<tr class="r1"><td class="c1">11/18/2022 </td><td class="c2"> CS Department Seminar at Virginia Tech </td></tr>
<tr class="r2"><td class="c1">10/14/2021 </td><td class="c2"> Invited talk at 2021 華仁全球講座 (Host: Jenq-Neng Hwang) </td></tr>
<tr class="r3"><td class="c1">08/26/2021 </td><td class="c2"> Invited talk at Graphics And Mixed Environment Seminar (Host: Jun-Yan Zhu)</td></tr>
<tr class="r4"><td class="c1">07/20/2021 </td><td class="c2"> Invited talk at School of Computer Science, Tel Aviv University (Host: Daniel Cohen-Or)</td></tr>
<tr class="r5"><td class="c1">07/13/2021 </td><td class="c2"> Invited talk at VGG seminar, University of Oxford </td></tr>
<tr class="r6"><td class="c1">07/07/2021 </td><td class="c2"> Invited talk at IMAGINE lab, Ecole des Ponts ParisTech </td></tr>
<tr class="r7"><td class="c1">05/10/2021 </td><td class="c2"> Invited talk at National Yang Ming Chiao Tung University </td></tr>
<tr class="r8"><td class="c1">04/23/2021 </td><td class="c2"> Invited talk at UT Austin (Host: Zhangyang Wang)</td></tr>
<tr class="r9"><td class="c1">04/20/2021 </td><td class="c2"> School of Interactive Computing Seminar at Georgia Tech (Host: Frank Dellaert) </td></tr>
<tr class="r10"><td class="c1">04/08/2021 </td><td class="c2"> CS Colloquium at Cornell University (Host: Noah Snavely) </td></tr>
<tr class="r11"><td class="c1">04/05/2021 </td><td class="c2"> ECE Seminar at the University of Michigan </td></tr>
<tr class="r12"><td class="c1">03/29/2021 </td><td class="c2"> CS Colloquium at University of Maryland (Host: Abhinav Shrivastava) </td></tr>
<tr class="r13"><td class="c1">03/22/2021 </td><td class="c2"> CS Colloquium at University of North Carolina Chapel Hill (Host: Mohit Bansal) </td></tr>
<tr class="r14"><td class="c1">02/26/2021 </td><td class="c2"> EECS Department Seminar at UC Merced (Host: Ming-Hsuan Yang)</td></tr>
<tr class="r15"><td class="c1">02/05/2021 </td><td class="c2"> 3M Non-Tenured Faculty Award Symposium </td></tr>
<tr class="r16"><td class="c1">02/02/2021 </td><td class="c2"> Visual Information Laboratory Seminar, University of Bristol (Host: Dima Damen) </td></tr>
<tr class="r17"><td class="c1">01/26/2021 </td><td class="c2"> Computer Vision Seminar, University of Illinois, Urbana-Champaign </td></tr>
<tr class="r18"><td class="c1">01/20/2021 </td><td class="c2"> <a href="https://yenchenlin.me/3D-representation-reading/">3D Representations Reading Group</a> at MIT [<a href="https://www.youtube.com/watch?v=kzmR3Njtwdg">Video</a>] [<a href="https://www.dropbox.com/s/tv8jvqlrv78dkvw/2021_01_20%20Neural%20Rendering%20of%20Dynamic%20Scenes.pptx">Slides</a>]</td></tr>
<tr class="r19"><td class="c1">
</td></tr></table>
<h2>Highlights and News</h2>
<table id="Highlights">
<tr class="r1"><td class="c1">09 / 2021 </td><td class="c2"> Thanks Adobe, Meta, Google for the gift donation. </td></tr>
<tr class="r2"><td class="c1">06 / 2021 </td><td class="c2"> Thanks Virginia’s Commonwealth Cyber Initiative for the grants on Detecting Disinformation and Misinformation (with <a href="https://ruoxijia.info/">Ruoxi Jia</a> <a href="https://liberalarts.vt.edu/departments-and-schools/department-of-communication/faculty/adrienne-ivory.html">Adrienne Ivory</a>)</td></tr>
<tr class="r3"><td class="c1">04 / 2021 </td><td class="c2"> Thanks Center for Human-Computer Interaction for the Planning Grants for Large-scale Research Efforts(with <a href="https://wordpress.cs.vt.edu/3digroup/author/dbowman/">Douglas Bowman</a>, <a href="https://www.ise.vt.edu/people/faculty/gabbard.html">Joe Gabbard</a>,<a href="https://www.bc.vt.edu/people/roofigari-esfahan">Nazila Roofigari-Esfahan</a>, and <a href="https://people.cs.vt.edu/hdardiry/">Hoda Eldardiry</a>) </td></tr>
<tr class="r6"><td class="c1">04 / 2021 </td><td class="c2"> Congrats to Esther and Meng-Li for getting into top PhD programs!Esther will join  CS@Stanford University. Meng-Li will join CSE @ University of Washington. </td></tr>
<tr class="r8"><td class="c1">08 / 2020 </td><td class="c2"> Thanks Facebook and Adobe for the gift donation. </td></tr>
<tr class="r9"><td class="c1">08 / 2020 </td><td class="c2"> Thank NSF for the Smart and Connected Health (SCH) grant (1.1M for four years). </td></tr>
<tr class="r10"><td class="c1">03 / 2020 </td><td class="c2"> Thank 3M for supporting our work with the 3M Non-Tenure Faculty Award. </td></tr>
<tr class="r11"><td class="c1">02 / 2020 </td><td class="c2"> Thanks 4-VA for the supporting us with a collaborative research grant (with <a href="http://vicenteordonez.com/">Vicente Ordonez</a> at University of Virginia). </td></tr>
<tr class="r12"><td class="c1">09 / 2019 </td><td class="c2"> Thanks NSF for supporting us with a medium Cyber-Physical Systems (CPS) grant (with <a href="https://caslab.ece.vt.edu/">Ryan K. Williams</a>, <a href="https://www.faculty.ece.vt.edu/zeng/">Haibo Zeng</a>, <a href="https://www.cs.purdue.edu/homes/chjung/">Changhee Jung</a>). </td></tr>
<tr class="r13"><td class="c1">09 / 2019 </td><td class="c2"> Thanks Rehabilitation Engineering Research Centers (RERC) for supporting us through the Rehabilitation Strategies, Techniques, and Interventions program (with <a href="https://www.sralab.org/">Shirley Ryan Ability Lab</a>, <a href="https://www.performingarts.vt.edu/index.php/faculty-staff/view/thanassis-rikakis">Thanassis Rikakis</a>, <a href="http://people.cs.vt.edu/~aislingk/">Aisling Kelliher</a>). </td></tr>
<tr class="r14"><td class="c1">04 / 2019 </td><td class="c2"> Thanks SAMSUNG for continuing to support our research through the <a href="https://www.sait.samsung.co.kr/saithome/about/collabo_overview.do">GRO award</a> (with <a href="http://alexander-schwing.de/">Alexander Schwing</a>). </td></tr>
<tr class="r15"><td class="c1">02 / 2019 </td><td class="c2"> Thanks Google for supporting our research with the <a href="https://ai.google/research/outreach/faculty-research-awards/">Google Faculty Research Award</a>. </td></tr>
<tr class="r16"><td class="c1">04 / 2018 </td><td class="c2"> Thanks NSF for supporting our research with an <a href="https://nsf.gov/awardsearch/showAward?AWD_ID=1755785&amp;HistoricalAwards=false">CRII award</a>. </td></tr>
<tr class="r17"><td class="c1">
</td></tr></table>
<h2>Recent Community Services</h2>
<ul>
<li><p><b>Computer Vision</b></p>
<ul>
<li><p>CVPR: Area Chair 2019; Reviewer 2020, 2021, 2022</p>
</li>
<li><p>ICCV: Area Chair 2019, 2021</p>
</li>
<li><p>ECCV: Area Chair 2022; Reviewer 2020</p>
</li>
<li><p>BMVC: Area Chair 2019, 2020, 2021, 2022</p>
</li>
<li><p>WACV: Area Chair 2020, 2022, 2023</p>
</li>
<li><p>IET Computer Vision: Associate Editor 2020-2022</p>
</li>
</ul>

</li>
</ul>
<ul>
<li><p><b>Computer Graphics</b></p>
<ul>
<li><p>SIGGRAPH: Technical Program Committee 2022, 2023</p>
</li>
<li><p>SIGGRAPH Asia: Technical Program Committee 2020, 2021</p>
</li>
<li><p>Eurographics: International Program Committee 2023</p>
</li>
<li><p>Computer Graphics Forum: Associate Editor 2021-2023</p>
</li>
</ul>

</li>
</ul>
<ul>
<li><p><b>Artificial Intelligence / Machine Learning</b></p>
<ul>
<li><p>NeurIPS: Area Chair 2021, 2022</p>
</li>
<li><p>ICML: Area Chair 2022, Reviewer 2021</p>
</li>
<li><p>ICLR: Reviewer 2020, 2022, 2023</p>
</li>
<li><p>AAAI: Area Chair 2023, Senior Program Committee 2021, 2022</p>
</li>
<li><p>TMLR: Action Editor 2022</p>
</li>
</ul>

</li>
</ul>
<ul>
<li><p><b>Mentoring</b></p>
<ul>
<li><p>Student Mentoring: CVPR 2021, 2022; ICCV 2021</p>
</li>
<li><p>Doctoral Consortium: ICCV 2021</p>
</li>
<li><p>LatinX in AI Mentoring Program: CVPR 2021, ICCV 2021</p>
</li>
<li><p>&ldquo;A Conversation With &hellip;&rdquo;: SIGGRAPH Asia 2021</p>
</li>
</ul>

</li>
</ul>
<p>
<a name="group"></a>
</p>
<h2>Research Group</h2>
<table id="Current-Students">
<tr class="r1"><td class="c1"><img class="alignmiddle" style="border-radius: 20%;" src="images/people/Badour_AlBahar.jpg" alt="Badour AlBahar" height="175" /> </td><td class="c2">
<img class="alignmiddle" style="border-radius: 20%;" src="images/people/Yiran_Xu.jpg" alt="Yiran Xu" height="175" /> </td><td class="c2">
<img class="alignmiddle" style="border-radius: 20%;" src="images/people/Yue_Feng.jpg" alt="Yue Feng" height="175" /></td></tr>
<tr class="r5"><td class="c1"><b><a href="https://badouralbahar.github.io/">Badour AlBahar</a></b> </td><td class="c2"><b><a href="https://twizwei.github.io/">Yiran Xu</a></b> </td><td class="c2"><b><a href="">Yue Feng</a></b> </td><td class="c2"></td></tr>
<tr class="r9"><td class="c1">PhD student </td><td class="c2"> PhD student </td><td class="c3"> PhD student  </td><td class="c4">  </td></tr>
<tr class="r10"><td class="c1">
</td></tr></table>
<h2>Alumni</h2>
<h3>Former PhD Students</h3>
<ul>
<li><p><a href="https://gaochen315.github.io/">Chen Gao</a> (PhD 2022), now Research Scientist at Meta</p>
</li>
<li><p><a href="https://yuliang-zou.github.io/index.html">Yuliang Zou</a> (PhD 2022), now Research Scientist at Waymo</p>
</li>
<li><p><a href="https://sites.google.com/site/jchoivision/">Jinwoo Choi</a> (PhD 2020), now an Assistant Professor at Kyung Hee Univeristy (Korea)</p>
</li>
</ul>
<h3>Former MS/BS Students</h3>
<ul>
<li><p><a href="http://www.estherrobb.com/">Esther Robb</a> (MS 2021), next a PhD student at Stanford University.</p>
</li>
<li><p><a href="https://josephcmessou.weebly.com/about.html">Joseph Messou</a> (MS 2020), next a PhD student at University of Maryland College Park.</p>
</li>
<li><p><a href="https://lemonatsu.github.io/home">Shih-Yang Su</a> (MS 2020), next a PhD student at University of British Columbia.</p>
</li>
<li><p><a href="https://www.linkedin.com/in/subhashree-radhakrishnan-b0b0048b/">Subhashree Radhakrishnan</a> (MS 2018), next a Deep Learning Software Engineeer at NVIDIA.</p>
</li>
<li><p><a href="https://phuang17.github.io/">Po-Han Huang</a> (MS 2018), next a Deep Learning Software Engineeer at NVIDIA.</p>
</li>
<li><p><a href="https://sanketloke.github.io/">Sanket Lokegaonkar</a> (MS 2018), next a Software Engineeer at Amazon AWS.</p>
</li>
<li><p><a href="">Adithya Nallabolu</a> (MS 2017), next a Computer Vision R&amp;D Engineer at Qualcomm.</p>
</li>
<li><p><a href="https://www.linkedin.com/in/le-wang-a8881999">Le Wang</a> (BS 2014), next MS at Stanford, now ASIC/RTL Designer at Google.</p>
</li>
<li><p><a href="https://www.linkedin.com/in/michael-qiu-0b47087a/">Michael Qiu</a> (BS 2014), next Senior Data Engineer at Capital One.</p>
</li>
<li><p><a href="http://www.linkedin.com/in/anarghya">Anarghya Mitra</a> (BS 2014), next Software Engineer at Google.</p>
</li>
<li><p><a href="http://alan.vision/">Zelun Luo</a> (BS 2013), next a PhD student at Stanford University.</p>
</li>
<li><p><a href="http://www.jgwak.com/">JunYoung Gwak</a> (BS 2013), next a PhD student at Stanford University.</p>
</li>
<li><p><a href="https://www.linkedin.com/in/danyang-wang-67291134">Danyang (Mike) Wang</a> (BS 2012), next Analog Design Engineer at Analog Devices.</p>
</li>
<li><p><a href="http://publish.illinois.edu/linjiachang/">Linjia Chang</a> (BS 2012), next MS at UIUC, now technical marketing engineer at Intel.</p>
</li>
<li><p><a href="https://www.ece.illinois.edu/directory/profile/srvstvv2">Sakshi Srivastava</a> (BS 2012), next PhD student at UIUC.</p>
</li>
<li><p><a href="http://www.classicallimit.com/">Kevin Han</a> (BS 2012), next PhD at UC Berkeley, now Senior Engineer at Pinnacle Photonics.</p>
</li>
</ul>
<h3>Former Visiting Students</h3>
<ul>
<li><p><a href="https://timy90022.github.io/about/">Ting-I Hsieh</a> (Intern 2020)</p>
</li>
<li><p><a href="https://yunchunchen.github.io/">Yun-Chun Chen</a> (Intern 2019), next PhD student at University of Toronto.</p>
</li>
<li><p><a href="https://hubert0527.github.io/">Chieh Hubert Lin</a> (Intern 2019), next PhD student at UC Merced.</p>
</li>
<li><p><a href="https://shihmengli.github.io/">Meng-Li Shih</a> (Intern 2018), next PhD student at University of Washington.</p>
</li>
<li><p><a href="https://markdtw.github.io/">Jin-Dong Dong</a> (Intern 2018), next PhD student at CMU.</p>
</li>
<li><p><a href="https://wyharveychen.github.io/">Wei-Yu Chen</a> (Intern 2018), next PhD student at CMU.</p>
</li>
<li><p><a href="https://gaochen315.github.io/">Chen Gao</a> (Intern 2018), next PhD student at Virginia Tech.</p>
</li>
<li><p><a href="https://yenchenlin.me/">Yen-Chen Lin</a> (Intern 2017), next PhD student at MIT.</p>
</li>
<li><p><a href="">Hao-Wei Yeh</a> (Intern 2017), next PhD student at University of Tokyo.</p>
</li>
</ul>
<p>
<a name="teaching"></a>
</p>
<h2>Teaching</h2>
<ul>
<li><p>Advanced Machine Learning: Spring 2021, Spring 2020, Spring 2019</p>
</li>
<li><p>Advanced Computer Vision: Spring 2017</p>
</li>
<li><p>Computer Vision: Fall 2018, Fall 2017, Fall 2016</p>
</li>
<li><p>Deep Learning: Fall 2020, Fall 2019</p>
</li>
<li><p>Introduction to Programming: Spring 2018</p>
</li>
</ul>
<p>
<a name="pubs"></a>
</p>
<h2>Publications</h2>
<h3>Preprints</h3>
<table class="imgtable"><tr><td>
<video id="vid" muted="muted" width="400" controls= "autoPlay" onended="this.currentTime = 0; this.play();" autoplay="" src="images/projects/arXiv 2020 PortraitNeRF.mp4"> <source src="images/projects/arXiv 2020 PortraitNeRF.mp4" type="video/mp4">
</video></td><td align="left">
<p><b>Portrait Neural Radiance Fields from a Single Image</b> <br />
<a href="http://chengao.vision/">Chen Gao</a>,
<a href="https://people.csail.mit.edu/yichangshih/">Yichang Shih</a>,
<a href="https://www.wslai.net/">Wei-Sheng Lai</a>,
<a href="http://chiakailiang.org/">Chia-Kai Liang</a>, and
<a href="http://www.jiabinhuang.com">Jia-Bin Huang</a> <br />
[<a href="https://arxiv.org/abs/2012.05903">Paper (PDF)</a>]
[<a href="https://portrait-nerf.github.io/">Project page</a>]

arXiv 2020 <br /></p>
<p>
<a class="github-button" href="" data-icon="octicon-star" data-size="large" data-show-count="true" aria-label="Star  on GitHub">Star</a>
<a class="github-button" href="" data-icon="octicon-repo-forked" data-size="large" data-show-count="true" aria-label="Fork  on GitHub">Fork</a>
 <br /><br />
Using meta-learning and 3D morphable models enables estimating portrait NeRF from a single image.</p>
</td></tr></table>
<table class="imgtable"><tr><td>
<video id="vid" width="400" controls= "autoPlay" onended="this.currentTime = 0; this.play();" autoplay="" src="images/projects/arXiv 2020 .mp4"> <source src="images/projects/arXiv 2020 .mp4" type="video/mp4">
</video></td><td align="left">
<p><b>Few-shot Adaptation of Generative Adversarial Networks</b> <br />
<a href="http://www.estherrobb.com/">Esther Robb</a>,
<a href="https://l2ior.github.io/">Vincent Chu</a>,
<a href="http://users.umiacs.umd.edu/~abhishek/">Abhishek Kumar</a>
<a href="http://www.jiabinhuang.com/">Jia-Bin Huang</a>
<br />
<i>arXiv 2020</i> <br />
[<a href="https://arxiv.org/abs/2010.11943">Paper (PDF)</a>]
[<a href="http://www.estherrobb.com/few-shot-gan/">Project page</a>]
[<a href="https://github.com/e-271/few-shot-gan">Code</a>]  <br /></p>
<p>
<a class="github-button" href="https://github.com/e-271/few-shot-gan" data-icon="octicon-star" data-size="large" data-show-count="true" aria-label="Star e-271/few-shot-gan on GitHub">Star</a>
<a class="github-button" href="https://github.com/e-271/few-shot-gan/fork" data-icon="octicon-repo-forked" data-size="large" data-show-count="true" aria-label="Fork e-271/few-shot-gan on GitHub">Fork</a>
 <br /><br /></p>
</td></tr></table>
<h3>Refereed publications</h3>
<table class="imgtable"><tr><td>
<img src="images/projects/TBA.jpg" alt="" width="400px" />&nbsp;</td>
<td align="left"><p><b>Temporally Consistent Semantic Video Editing</b> <br />
<a href="">Yiran Xu</a>,
<a href="">Badour AlBahar</a>,
<a href="">Jia-Bin Huang</a> <br />
/Proceedings of European Conference on Computer Vision (ECCV), 2022 <br />
[<a href="">Paper (PDF)</a>]
[<a href="">Project page</a>]
[<a href="">Code</a>]
[<a href="">Poster</a>]
[<a href="">Supp</a>]</p>
<p>
<a class="github-button" href="https://github.com/" data-icon="octicon-star" data-size="large" data-show-count="true" aria-label="Star on GitHub">Star</a>
<a class="github-button" href="https://github.com/fork" data-icon="octicon-repo-forked" data-size="large" data-show-count="true" aria-label="Fork on GitHub">Fork</a>
</p>
</td></tr></table>
<table class="imgtable"><tr><td>
<img src="images/projects/TBA.jpg" alt="" width="400px" />&nbsp;</td>
<td align="left"><p><b>Learning Instance-Specific Adaptation for Cross-Domain Segmentation</b> <br />
<a href="">Yuliang Zou</a>,
<a href="">Zizhao Zhang</a>,
<a href="">Chun-Liang Li</a>,
<a href="">Han Zhang</a>,
<a href="">Tomas Pfister</a>,
<a href="">Jia-Bin Huang</a> <br />
/Proceedings of European Conference on Computer Vision (ECCV), 2022 <br />
[<a href="">Paper (PDF)</a>]
[<a href="">Project page</a>]
[<a href="">Code</a>]
[<a href="">Poster</a>]
[<a href="">Supp</a>]</p>
<p>
<a class="github-button" href="https://github.com/" data-icon="octicon-star" data-size="large" data-show-count="true" aria-label="Star on GitHub">Star</a>
<a class="github-button" href="https://github.com/fork" data-icon="octicon-repo-forked" data-size="large" data-show-count="true" aria-label="Fork on GitHub">Fork</a>
</p>
</td></tr></table>
<table class="imgtable"><tr><td>
<img src="images/projects/TBA.jpg" alt="" width="400px" />&nbsp;</td>
<td align="left"><p><b>Long Video Generation with Time-Agnostic VQGAN and Time-Sensitive Transformer</b> <br />
<a href="">Songwei Ge</a>,
<a href="">Thomas Hayes</a>,
<a href="">Harry Yang</a>,
<a href="">Xi Yin</a>,
<a href="">Guan Pang</a>,
<a href="">David Jacobs</a>,
<a href="">Jia-Bin Huang</a>,
<a href="">Devi Parikh</a> <br />
/Proceedings of European Conference on Computer Vision (ECCV), 2022 <br />
[<a href="">Paper (PDF)</a>]
[<a href="">Project page</a>]
[<a href="">Code</a>]
[<a href="">Poster</a>]
[<a href="">Supp</a>]</p>
<p>
<a class="github-button" href="https://github.com/" data-icon="octicon-star" data-size="large" data-show-count="true" aria-label="Star on GitHub">Star</a>
<a class="github-button" href="https://github.com/fork" data-icon="octicon-repo-forked" data-size="large" data-show-count="true" aria-label="Fork on GitHub">Fork</a>
</p>
</td></tr></table>
<table class="imgtable"><tr><td>
<img src="images/projects/TBA.jpg" alt="" width="400px" />&nbsp;</td>
<td align="left"><p><b>Boosting View Synthesis with Residual Transfer</b> <br />
<a href="https://xrong.org/">Xuejian Rong</a>,
<a href="http://www.jiabinhuang.com">Jia-Bin Huang</a>,
<a href="https://www.linkedin.com/in/ayush29feb/">Ayush Saraf</a>,
<a href="https://xrong.org/">Xuejian Rong</a>,
<a href="https://www.linkedin.com/in/ayush29feb/">Ayush Saraf</a>,
<a href="https://people.csail.mit.edu/changil/">Changil Kim</a>,
<a href="https://johanneskopf.de/">Johannes Kopf</a> <br />
/Proceedings of IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), 2022 <br />
[<a href="">Paper (PDF)</a>]
[<a href="">Project page</a>]
[<a href="">Code</a>]
[<a href="">Poster</a>]
[<a href="">Supp</a>]</p>
<p>
<a class="github-button" href="https://github.com/" data-icon="octicon-star" data-size="large" data-show-count="true" aria-label="Star on GitHub">Star</a>
<a class="github-button" href="https://github.com/fork" data-icon="octicon-repo-forked" data-size="large" data-show-count="true" aria-label="Fork on GitHub">Fork</a>
</p>
</td></tr></table>
<table class="imgtable"><tr><td>
<img src="images/projects/TBA.jpg" alt="" width="400px" />&nbsp;</td>
<td align="left"><p><b>Learning Neural Light Fields with Ray-Space Embedding Networks</b> <br />
<a href="https://www.battal.me/">Benjamin Attal</a>,
<a href="http://www.jiabinhuang.com">Jia-Bin Huang</a>,
<a href="https://zollhoefer.com/">Michael Zollhöfer</a>,
<a href="https://johanneskopf.de/">Johannes Kopf</a>, and
<a href="https://people.csail.mit.edu/changil/">Changil Kim</a> <br />
/Proceedings of IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), 2022 <br />
[<a href="">Paper (PDF)</a>]
[<a href="">Project page</a>]
[<a href="Code">Code</a>]
[<a href="">Poster</a>]
[<a href="">Supp</a>]</p>
<p>
<a class="github-button" href="https://github.com/" data-icon="octicon-star" data-size="large" data-show-count="true" aria-label="Star on GitHub">Star</a>
<a class="github-button" href="https://github.com/fork" data-icon="octicon-repo-forked" data-size="large" data-show-count="true" aria-label="Fork on GitHub">Fork</a>
</p>
</td></tr></table>
<table class="imgtable"><tr><td>
<img src="images/projects/TBA.jpg" alt="" width="400px" />&nbsp;</td>
<td align="left"><p><b>Neural Global Shutter: Learn to Restore Video from a Rolling Shutter Camera with Global Reset Feature</b> <br />
<a href="">Zhixiang Wang</a>,
<a href="">Xiang Ji</a>,
<a href="http://www.jiabinhuang.com">Jia-Bin Huang</a>,
<a href="">Shin'ichi Satoh</a>,
<a href="">Xiao Zhou</a>, and
<a href="">Yinqiang Zheng</a><br />
/Proceedings of IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), 2022 <br />
[<a href="">Paper (PDF)</a>]
[<a href="">Project page</a>]
[<a href="Code">Code</a>]
[<a href="">Poster</a>]
[<a href="">Supp</a>]</p>
<p>
<a class="github-button" href="https://github.com/" data-icon="octicon-star" data-size="large" data-show-count="true" aria-label="Star on GitHub">Star</a>
<a class="github-button" href="https://github.com/fork" data-icon="octicon-repo-forked" data-size="large" data-show-count="true" aria-label="Fork on GitHub">Fork</a>
</p>
</td></tr></table>
<table class="imgtable"><tr><td>
<video id="vid" muted="muted" width="400" controls= "autoPlay" onended="this.currentTime = 0; this.play();" autoplay="" src="images/projects/arXiv 2020 SAVI2I.mp4"> <source src="images/projects/arXiv 2020 SAVI2I.mp4" type="video/mp4">
</video></td><td align="left">
<p><b>Continuous and Diverse Image-to-Image Translation via Signed Attribute Vectors</b> <br />
<a href="https://sites.google.com/view/qi-mao/">Qi Mao</a>,
<a href="http://vllab.ucmerced.edu/hylee/">Hsin-Ying Lee</a>,
<a href="https://sites.google.com/site/hytseng0509/">Hung-Yu Tseng</a>,
<a href="http://www.jiabinhuang.com/">Jia-Bin Huang</a>,
<a href="https://eecs.pku.edu.cn/info/1468/9643.htm">Siwei Ma</a>, and
<a href="https://faculty.ucmerced.edu/mhyang/">Ming-Hsuan Yang</a> <br />
<i>International Journal of Computer Vision (IJCV), 2022</i> <br />
[<a href="https://arxiv.org/abs/2011.01215">Paper (PDF)</a>]
[<a href="https://helenmao.github.io/SAVI2I/">Project page</a>]
[<a href="https://github.com/HelenMao/SAVI2I">Code</a>] <br /></p>
<p>
<a class="github-button" href="https://github.com/HelenMao/SAVI2I" data-icon="octicon-star" data-size="large" data-show-count="true" aria-label="Star HelenMao/SAVI2I on GitHub">Star</a>
<a class="github-button" href="https://github.com/HelenMao/SAVI2I/fork" data-icon="octicon-repo-forked" data-size="large" data-show-count="true" aria-label="Fork HelenMao/SAVI2I on GitHub">Fork</a>
 <br /></p>
</td></tr></table>
<table class="imgtable"><tr><td>
<video id="vid" muted="muted" width="400" controls= "autoPlay" onended="this.currentTime = 0; this.play();" autoplay="" src="images/projects/SigAsia 2021 PoseWithStyle.mp4"> <source src="images/projects/SigAsia 2021 PoseWithStyle.mp4" type="video/mp4">
</video></td><td align="left">
<p><b> Pose with Style: Detail-Preserving Pose-Guided Image Synthesis with Conditional StyleGAN</b> <br />
<a href="https://badouralbahar.github.io/">Badour AlBahar</a>,
<a href="https://research.adobe.com/person/jingwan-lu/">Jingwan (Cynthia) Lu</a>,
<a href="https://eng.ucmerced.edu/people/jyang44">Jimei Yang</a>,
<a href="https://zhixinshu.github.io/">Zhixin Shu</a>,
<a href="https://research.adobe.com/person/eli-shechtman/">Eli Shechtman</a>, and
<a href="https://jbhuang0604.github.io/">Jia-Bin Huang</a> <br />
<i>ACM Transactions on Graphics (Proceedings of SIGGRAPH Asia), 2021 </i> <br />
[<a href="https://pose-with-style.github.io/asset/paper.pdf">Paper (PDF)</a>]
[<a href="https://pose-with-style.github.io/">Project page</a>]
[<a href="https://github.com/BadourAlBahar/pose-with-style">Code</a>]
[<a href="https://colab.research.google.com/github/tg-bomze/collection-of-notebooks/blob/master/HomeStylist.ipynb">Colab notebook</a>] <br /></p>
<p>
<a class="github-button" href="https://github.com/" data-icon="octicon-star" data-size="large" data-show-count="true" aria-label="Star on GitHub">Star</a>
<a class="github-button" href="https://github.com/fork" data-icon="octicon-repo-forked" data-size="large" data-show-count="true" aria-label="Fork on GitHub">Fork</a>
 <br /><br />
Modulating StyleGAN for photorealistic human reposing and virtual try-on.</p>
</td></tr></table>
<table class="imgtable"><tr><td>
<video id="vid" width="400" controls= "autoPlay" onended="this.currentTime = 0; this.play();" autoplay="" src="images/projects/arXiv 2020 Obstruction.mp4"> <source src="images/projects/arXiv 2020 Obstruction.mp4" type="video/mp4">
</video></td><td align="left">
<p><b>Learning to See Through Obstructions with Layered Decomposition</b> <br />
<a href="http://www.cmlab.csie.ntu.edu.tw/~yulunliu/">Yu-Lun Liu</a>,
<a href="https://www.wslai.net/">Wei-Sheng Lai</a>,
<a href="https://faculty.ucmerced.edu/mhyang/">Ming-Hsuan Yang</a>,
<a href="https://www.csie.ntu.edu.tw/~cyy/">Yung-Yu Chuang</a>, and
<a href="http://www.jiabinhuang.com">Jia-Bin Huang</a><br />
<i>IEEE Transactions on Pattern Analysis and Machine Intelligence (PAMI), 2021</i> <br />
[<a href="https://arxiv.org/abs/2008.04902">Paper (PDF)</a>]
[<a href="https://alex04072000.github.io/SOLD/">Project page</a>]
[<a href="https://github.com/alex04072000/SOLD">Code</a>]
[<a href="https://colab.research.google.com/drive/1kCG5SJd3usgzi6Bx979KiaO_YTanNVVz?usp=sharing">Colab notebook</a>] <br />
News coverage: [<a href="https://www.newscientist.com/article/2253195-ai-removes-unwanted-objects-from-photos-to-give-a-clearer-view/">New Scientists</a>]</p>
<p>
<a class="github-button" href="https://github.com/alex04072000/SOLD" data-icon="octicon-star" data-size="large" data-show-count="true" aria-label="Star alex04072000/SOLD on GitHub">Star</a>
<a class="github-button" href="https://github.com/alex04072000/SOLD/fork" data-icon="octicon-repo-forked" data-size="large" data-show-count="true" aria-label="Fork alex04072000/SOLD on GitHub">Fork</a>
 <br /><br />
Using meta-learning to enable faster layered decomposition with test-time optimization.</p>
</td></tr></table>
<table class="imgtable"><tr><td>
<img src="images/projects/TBA.jpg" alt="" width="400px" />&nbsp;</td>
<td align="left"><p><b>AMICO: Amodal Instance Composition</b> <br />
<a href="https://payeah.net/">Peiye Zhuang</a>,
<a href="http://www.jiabinhuang.com">Jia-Bin Huang</a>,
<a href="https://www.linkedin.com/in/ayush29feb/">Ayush Saraf</a>,
<a href="https://xrong.org/">Xuejian Rong</a>,
<a href="https://people.csail.mit.edu/changil/">Changil Kim</a>,
<a href="https://www.linkedin.com/in/demandolx/">Denis Demandolx</a> <br />
<i>Proceedings of British Machine Vision Conference (BMVC), 2021</i> <br />
[<a href="">Paper (PDF)</a>]
[<a href="">Project page</a>]
[<a href="Code">Code</a>]
[<a href="">Poster</a>]
[<a href="">Supp</a>]</p>
<p>
<a class="github-button" href="https://github.com/" data-icon="octicon-star" data-size="large" data-show-count="true" aria-label="Star on GitHub">Star</a>
<a class="github-button" href="https://github.com/fork" data-icon="octicon-repo-forked" data-size="large" data-show-count="true" aria-label="Fork on GitHub">Fork</a>
</p>
</td></tr></table>
<table class="imgtable"><tr><td>
<video id="vid" muted="muted" width="400" controls= "autoPlay" onended="this.currentTime = 0; this.play();" autoplay="" src="images/projects/arXiv 2021 FreeViewVideo.mp4"> <source src="images/projects/arXiv 2021 NeRViS.mp4" type="video/mp4">
</video></td><td align="left">
<p><b>Dynamic View Synthesis from Dynamic Monocular Video</b> <br />
<a href="http://chengao.vision/">Chen Gao</a>,
<a href="https://www.linkedin.com/in/ayush29feb/">Ayush Saraf</a>,
<a href="http://johanneskopf.de/">Johannes Kopf</a>, and
<a href="http://www.jiabinhuang.com">Jia-Bin Huang</a><br />
[<a href="https://arxiv.org/abs/2105.06468">Paper (PDF)</a>]
[<a href="https://free-view-video.github.io/">Project page</a>]
[<a href="https://github.com/gaochen315/DynamicNeRF">Code</a>] <br />
<i>Proceedings of International Conference on Computer Vision (ICCV), 2021</i> <br /></p>
<p>
<a class="github-button" href="https://github.com/" data-icon="octicon-star" data-size="large" data-show-count="true" aria-label="Star  on GitHub">Star</a>
<a class="github-button" href="https://github.com/fork" data-icon="octicon-repo-forked" data-size="large" data-show-count="true" aria-label="Fork on GitHub">Fork</a>
 <br /><br />
Regularizing scene flow and depth to disabmiguate learning space-time neural radiance field.</p>
</td></tr></table>
<table class="imgtable"><tr><td>
<video id="vid" muted="muted" width="400" controls= "autoPlay" onended="this.currentTime = 0; this.play();" autoplay="" src="images/projects/arXiv 2021 NeRViS.mp4"> <source src="images/projects/arXiv 2021 NeRViS.mp4" type="video/mp4">
</video></td><td align="left">
<p><b>Hybrid Neural Fusion for Full-frame Video Stabilization</b> <br />
<a href="http://www.cmlab.csie.ntu.edu.tw/~yulunliu/">Yu-Lun Liu</a>,
<a href="https://www.wslai.net/">Wei-Sheng Lai</a>,
<a href="https://faculty.ucmerced.edu/mhyang/">Ming-Hsuan Yang</a>,
<a href="https://www.csie.ntu.edu.tw/~cyy/">Yung-Yu Chuang</a>, and
<a href="http://www.jiabinhuang.com">Jia-Bin Huang</a><br />
[<a href="https://arxiv.org/abs/2102.06205">Paper (PDF)</a>]
[<a href="https://alex04072000.github.io/FuSta/">Project page</a>]
[<a href="https://github.com/alex04072000/FuSta">Code</a>]
[<a href="https://colab.research.google.com/drive/1l-fUzyM38KJMZyKMBWw_vu7ZUyDwgdYH?usp=sharing">Colab notebook</a>]
[<a href="https://www.youtube.com/watch?v=v5pOsQEOsyA">Two minute video</a>] <br />
<i>Proceedings of International Conference on Computer Vision (ICCV), 2021</i> <br /></p>
<p>
<a class="github-button" href="https://github.com/alex04072000/FuSta" data-icon="octicon-star" data-size="large" data-show-count="true" aria-label="Star alex04072000/FuSta on GitHub">Star</a>
<a class="github-button" href="https://github.com/alex04072000/FuSta/fork" data-icon="octicon-repo-forked" data-size="large" data-show-count="true" aria-label="Fork alex04072000/FuSta on GitHub">Fork</a>
 <br /><br />
Warping and fusing multiple frames stabilize shaky videos without cropping.</p>
</td></tr></table>
<table class="imgtable"><tr><td>
<img src="images/projects/Neurology 2021 Rehabilitation.jpg" alt="" width="400px" />&nbsp;</td>
<td align="left"><p><b>Automated Movement Assessment in Stroke Rehabilitation</b> <br />
<a href="https://www.linkedin.com/in/-tamim-ahmed">Tamim Ahmed</a>,
<a href="https://kowshikthopalli.github.io/">Kowshik Thopalli</a>,
<a href="https://thanassisrikakis.com/">Thanassis Rikakis</a>,
<a href="https://pavanturaga.com/">Pavan Turaga</a>,
<a href="https://people.cs.vt.edu/~aislingk/">Aisling Kelliher</a>,
<a href="http://www.jiabinhuang.com">Jia-Bin Huang</a>, and
<a href="http://www.rehabmed.emory.edu/faculty.bios/wolf-steven.html">Steven L. Wolf</a>
<br />
<i>Frontiers in Neurology 2021</i> <br />
[<a href="https://www.frontiersin.org/articles/10.3389/fneur.2021.720650/full">Paper (PDF)</a>] <br /></p>
</td></tr></table>
<table class="imgtable"><tr><td>
<video id="vid" muted="muted" width="400" controls= "autoPlay" onended="this.currentTime = 0; this.play();" autoplay="" src="images/projects/arXiv 2020 RobustCVD.mp4"> <source src="images/projects/arXiv 2020 RobustCVD.mp4" type="video/mp4">
</video></td><td align="left">
<p><b>Robust Consistent Video Depth Estimation</b> <br />
<a href="http://johanneskopf.de/">Johannes Kopf</a>,
<a href="https://xrong.org/">Xuejian Rong</a> and
<a href="http://www.jiabinhuang.com">Jia-Bin Huang</a> <br />
<i>Proceedings of IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2021 (Oral Presentation)</i> <br />
[<a href="https://arxiv.org/abs/2012.05901">Paper (PDF)</a>]
[<a href="https://robust-cvd.github.io/">Project page</a>]
[<a href="https://github.com/facebookresearch/robust_cvd">Code</a>]
[<a href="https://colab.research.google.com/drive/1YOLXsb4JUOD1wt5TXB2ln78koocm-bu8?usp=sharing">Colab</a>]  <br /></p>
<p>
<a class="github-button" href="https://github.com/facebookresearch/robust_cvd" data-icon="octicon-star" data-size="large" data-show-count="true" aria-label="Star facebookresearch/robust_cvd on GitHub">Star</a>
<a class="github-button" href="https://github.com/facebookresearch/robust_cvd/fork" data-icon="octicon-repo-forked" data-size="large" data-show-count="true" aria-label="Fork facebookresearch/robust_cvd on GitHub">Fork</a>
 <br /><br />
Estimating geometrically consistent depth and camera poses from casually captured videos.</p>
</td></tr></table>
<table class="imgtable"><tr><td>
<video id="vid" muted="muted" width="400" controls= "autoPlay" onended="this.currentTime = 0; this.play();" autoplay="" src="images/projects/arXiv 2020 VideoNeRF.mp4"> <source src="images/projects/arXiv 2020 VideoNeRF.mp4" type="video/mp4">
</video></td><td align="left">
<p><b>Space-time Neural Irradiance Fields for Free-Viewpoint Video</b> <br />
<a href="http://www.cs.cornell.edu/~wenqixian/">Wenqi Xian</a>,
<a href="http://www.jiabinhuang.com">Jia-Bin Huang</a>,
<a href="https://johanneskopf.de/">Johannes Kopf</a>, and
<a href="https://people.csail.mit.edu/changil/">Changil Kim</a> <br />
<i>Proceedings of IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2021</i> <br />
[<a href="https://arxiv.org/abs/2011.12950">Paper (PDF)</a>]
[<a href="https://video-nerf.github.io/">Project page</a>]
[<a href="">Code</a>] <br /></p>
<p>
<a class="github-button" href="" data-icon="octicon-star" data-size="large" data-show-count="true" aria-label="Star  on GitHub">Star</a>
<a class="github-button" href="" data-icon="octicon-repo-forked" data-size="large" data-show-count="true" aria-label="Fork  on GitHub">Fork</a>
 <br /><br />
Learning spatiotemporal NeRF for dynamic scenes from a single video is ambiguious, but we can constrain the disambiute it using scene depth.</p>
</td></tr></table>
<table class="imgtable"><tr><td>
<img src="images/projects/arXiv 2020 PseudoSeg.jpg" alt="" width="400px" />&nbsp;</td>
<td align="left"><p><b>PseudoSeg: Designing Pseudo Labels for Semantic Segmentation</b> <br />
<a href="https://yuliang.vision/">Yuliang Zou</a>,
<a href="https://sites.google.com/view/zizhaozhang/home">Zizhao Zhang</a>,
<a href="https://sites.google.com/view/hanzhang">Han Zhang</a>,
<a href="https://chunliangli.github.io/">Chun-Liang Li</a>,
<a href="https://www.linkedin.com/in/xiao-sean-bian-14b06495/">Xiao Bian</a>,
<a href="http://www.jiabinhuang.com/">Jia-Bin Huang</a>,
<a href="https://tomas.pfister.fi/">Tomas Pfister</a><br />
<i>International Conference on Learning Representations (ICLR), 2021</i> <br />
[<a href="https://arxiv.org/abs/2010.09713">Paper (PDF)</a>]
[<a href="https://yuliang.vision/pseudo_seg/">Project page</a>]
[<a href="https://github.com/googleinterns/wss">Code</a>] <br /></p>
<p>
<a class="github-button" href="https://github.com/googleinterns/wss" data-icon="octicon-star" data-size="large" data-show-count="true" aria-label="Star googleinterns/wss on GitHub">Star</a>
<a class="github-button" href="https://github.com/googleinterns/wss/fork" data-icon="octicon-repo-forked" data-size="large" data-show-count="true" aria-label="Fork googleinterns/wss on GitHub">Fork</a>
 <br /></p>
</td></tr></table>
<table class="imgtable"><tr><td>
<img src="images/projects/AAAI 2021 DropLoss.jpg" alt="" width="400px" />&nbsp;</td>
<td align="left"><p><b>DropLoss for Long-Tail Instance Segmentation</b> <br />
<a href="https://timy90022.github.io//">Ting-I Hsieh</a>*,
<a href="http://www.estherrobb.com/">Esther Robb</a>*,
<a href="https://htchen.github.io/">Hwann-Tzong Chen</a>, and
<a href="http://www.jiabinhuang.com/">Jia-Bin Huang</a> <br />
<i>Proceedings of AAAI Conference on Artificial Intelligence (AAAI), 2021</i> <br />
[<a href="https://arxiv.org/abs/2104.06402">Paper (PDF)</a>]
[<a href="https://github.com/timy90022/DropLoss">Project page</a>]
[<a href="https://github.com/timy90022/DropLoss">Code</a>]  <br /></p>
<p>
<a class="github-button" href="https://github.com/timy90022/DropLoss" data-icon="octicon-star" data-size="large" data-show-count="true" aria-label="Star timy90022/DropLoss on GitHub">Star</a>
<a class="github-button" href="https://github.com/timy90022/DropLoss/fork" data-icon="octicon-repo-forked" data-size="large" data-show-count="true" aria-label="Fork timy90022/DropLoss on GitHub">Fork</a>
 <br /></p>
</td></tr></table>
<table class="imgtable"><tr><td>
<video id="vid" muted="muted" width="400" controls= "autoPlay" onended="this.currentTime = 0; this.play();" autoplay="" src="images/projects/ECCV 2020 VideoCompletion.mp4">
<source src="images/projects/ECCV 2020 VideoCompletion.mp4" type="video/mp4">
</video></td><td align="left">
<p><b>Flow-edge Guided Video Completion</b> <br />
<a href="http://chengao.vision/">Chen Gao</a>,
<a href="https://www.linkedin.com/in/ayush29feb">Ayush Saraf</a>,
<a href="http://www.jiabinhuang.com">Jia-Bin Huang</a>, and
<a href="https://johanneskopf.de/">Johannes Kopf</a> <br />
<i>Proceedings of European Conference on Computer Vision (ECCV), 2020</i> <br />
[<a href="https://arxiv.org/abs/2009.01835">Paper (PDF)</a>]
[<a href="http://chengao.vision/FGVC/">Project page</a>]
[<a href="https://github.com/vt-vl-lab/FGVC">Code</a>]
[<a href="">Colab notebook</a>]
[<a href="https://www.youtube.com/watch?v=86QU7_SF16Q">Two minute video</a>] <br />
News coverage: [<a href="https://www.newscientist.com/article/2254351-ai-tool-improves-video-footage-by-editing-out-unwanted-objects/">New Scientists</a>]
[<a href="https://syncedreview.com/2020/09/08/virginia-tech-facebook-video-completion-algorithm-achieves-sota-results/">Syned Review</a>]</p>
<p>
<a class="github-button" href="https://github.com/vt-vl-lab/FGVC" data-icon="octicon-star" data-size="large" data-show-count="true" aria-label="Star vt-vl-lab/FGVC on GitHub">Star</a>
<a class="github-button" href="https://github.com/vt-vl-lab/FGVC" data-icon="octicon-repo-forked" data-size="large" data-show-count="true" aria-label="Fork vt-vl-lab/FGVC on GitHub">Fork</a>
</p>
</td></tr></table>
<table class="imgtable"><tr><td>
<video id="vid" muted="muted" width="400" controls= "autoPlay" onended="this.currentTime = 0; this.play();" autoplay="" src="images/projects/ECCV 2020 SemanticViewSynthesis.mp4">
<source src="images/projects/ECCV 2020 SemanticViewSynthesis.mp4" type="video/mp4">
</video></td><td align="left">
<p><b>
Semantic View Synthesis
</b> <br />
<a href="https://hhsinping.github.io/">Hsin-Ping Huang</a>,
<a href="https://sites.google.com/site/hytseng0509/">Hung-Yu Tseng</a>,
<a href="http://vllab.ucmerced.edu/hylee/">Hsin-Ying Lee</a>, and
<a href="http://www.jiabinhuang.com">Jia-Bin Huang</a>
<br />
<i>Proceedings of European Conference on Computer Vision (ECCV), 2020</i> <br />
[<a href="https://arxiv.org/abs/2008.10598">Paper (PDF)</a>]
[<a href="https://hhsinping.github.io/svs/index.html">Project page</a>]
[<a href="https://github.com/hhsinping/svs">Code</a>]
[<a href="https://colab.research.google.com/drive/1iT5PfK7zl1quAOwC227GfBjieFMVHjI5">Colab notebook</a>]</p>
<p>
<a class="github-button" href="https://github.com/hhsinping/svs" data-icon="octicon-star" data-size="large" data-show-count="true" aria-label="Star hhsinping/svs on GitHub">Star</a>
<a class="github-button" href="https://github.com/hhsinping/svs/fork" data-icon="octicon-repo-forked" data-size="large" data-show-count="true" aria-label="Fork hhsinping/svs on GitHub">Fork</a>
</p>
</td></tr></table>
<table class="imgtable"><tr><td>
<video id="vid" muted="muted" width="400" controls= "autoPlay" onended="this.currentTime = 0; this.play();" autoplay="" src="images/projects/ECCV 2020 HOI.mp4">
<source src="images/projects/ECCV 2020 HOI.mp4" type="video/mp4">
</video></td><td align="left">
<p><b>DRG: Dual Relation Graph for Human-Object Interaction Detection</b> <br />
<a href="http://chengao.vision/">Chen Gao</a>,
<a href="https://www.linkedin.com/in/jiaruixu-vt">Jiarui Xu</a>,
<a href="https://yuliang.vision/">Yuliang Zou</a>, and
<a href="http://www.jiabinhuang.com">Jia-Bin Huang</a> <br />
<i>Proceedings of European Conference on Computer Vision (ECCV), 2020</i> <br />
[<a href="https://arxiv.org/abs/2008.11714">Paper (PDF)</a>]
[<a href="http://chengao.vision/DRG/">Project page</a>]
[<a href="https://github.com/vt-vl-lab/DRG">Code</a>]
[<a href="">Colab notebook</a>]</p>
<p>
<a class="github-button" href="https://github.com/vt-vl-lab/DRG" data-icon="octicon-star" data-size="large" data-show-count="true" aria-label="Star vt-vl-lab/DRG on GitHub">Star</a>
<a class="github-button" href="https://github.com/vt-vl-lab/DRG" data-icon="octicon-repo-forked" data-size="large" data-show-count="true" aria-label="Fork vt-vl-lab/DRG on GitHub">Fork</a>
</p>
</td></tr></table>
<table class="imgtable"><tr><td>
<video id="vid" muted="muted" width="400" controls= "autoPlay" onended="this.currentTime = 0; this.play();" autoplay="" src="images/projects/ECCV 2020 NAS-DIP.mp4">
<source src="images/projects/ECCV 2020 NAS-DIP.mp4" type="video/mp4">
</video></td><td align="left">
<p><b>
NAS-DIP: Learning Deep Image Prior with Neural Architecture Search
</b> <br />
<a href="https://yunchunchen.github.io/">Yun-Chun Chen</a>*,
<a href="http://chengao.vision/">Chen Gao</a>*,
<a href="http://www.estherrobb.com/">Esther Robb</a>, and
<a href="http://www.jiabinhuang.com">Jia-Bin Huang</a>
<br />
<i>Proceedings of European Conference on Computer Vision (ECCV), 2020</i> <br />
[<a href="https://arxiv.org/abs/2008.11713">Paper (PDF)</a>]
[<a href="https://yunchunchen.github.io/NAS-DIP/">Project page</a>]
[<a href="https://github.com/YunChunChen/NAS-DIP-pytorch">Code</a>]
[<a href="https://colab.research.google.com/drive/1BhmZMeyGGP_T5SLPdLlkGUZLhrnO1FdF?usp=sharing">Colab notebook</a>]</p>
<p>
<a class="github-button" href="https://github.com/YunChunChen/NAS-DIP-pytorch" data-icon="octicon-star" data-size="large" data-show-count="true" aria-label="Star YunChunChen/NAS-DIP-pytorch on GitHub">Star</a>
<a class="github-button" href="https://github.com/YunChunChen/NAS-DIP-pytorch/fork" data-icon="octicon-repo-forked" data-size="large" data-show-count="true" aria-label="Fork YunChunChen/NAS-DIP-pytorch on GitHub">Fork</a>
</p>
</td></tr></table>
<table class="imgtable"><tr><td>
<img src="images/projects/ECCV 2020 FeatMatch.jpg" alt="" width="400px" />&nbsp;</td>
<td align="left"><p><b>FeatMatch: Feature-Based Augmentation for Semi-Supervised Learning</b> <br />
<a href="https://sites.google.com/view/chiawen-kuo/home">Chia-Wen Kuo</a>,
<a href="https://chihyaoma.github.io/">Chih-Yao Ma</a>,
<a href="http://www.jiabinhuang.com">Jia-Bin Huang</a>, and
<a href="https://www.cc.gatech.edu/~zk15/">Zsolt Kira</a>
<br />
<i>Proceedings of European Conference on Computer Vision (ECCV), 2020</i> <br />
[<a href="https://arxiv.org/abs/2007.08505">Paper (PDF)</a>]
[<a href="https://sites.google.com/view/chiawen-kuo/home/featmatch">Project page</a>]
[<a href="">Code</a>]
[<a href="">Colab notebook</a>] (Coming soon)</p>
<p>
<a class="github-button" href="" data-icon="octicon-star" data-size="large" data-show-count="true" aria-label="Star vt-vl-lab/3d-photo-inpainting on GitHub">Star</a>
<a class="github-button" href="" data-icon="octicon-repo-forked" data-size="large" data-show-count="true" aria-label="Fork vt-vl-lab/3d-photo-inpainting on GitHub">Fork</a>
</p>
</td></tr></table>
<table class="imgtable"><tr><td>
<img src="images/projects/ECCV 2020 VideoDA.jpg" alt="" width="400px" />&nbsp;</td>
<td align="left"><p><b>Shuffle and Attend: Video Domain Adaptation</b> <br />
<a href="https://sites.google.com/site/jchoivision/">Jinwoo Choi</a>,
<a href="https://grvsharma.com/">Gaurav Sharma</a>,
<a href="https://samschulter.github.io/">Samuel Schulter</a>, and
<a href="http://www.jiabinhuang.com">Jia-Bin Huang</a>
<br />
<i>Proceedings of European Conference on Computer Vision (ECCV), 2020</i> <br />
[<a href="http://www.ecva.net/papers/eccv_2020/papers_ECCV/papers/123570664.pdf">Paper (PDF)</a>]
[<a href="">Project page</a>]
[<a href="">Code</a>]
[<a href="">Colab notebook</a>] (Coming soon)</p>
<p>
<a class="github-button" href="" data-icon="octicon-star" data-size="large" data-show-count="true" aria-label="Star vt-vl-lab/3d-photo-inpainting on GitHub">Star</a>
<a class="github-button" href="" data-icon="octicon-repo-forked" data-size="large" data-show-count="true" aria-label="Fork vt-vl-lab/3d-photo-inpainting on GitHub">Fork</a>
</p>
</td></tr></table>
<table class="imgtable"><tr><td>
<video id="vid" muted="muted" width="400" controls= "autoPlay" onended="this.currentTime = 0; this.play();" autoplay="" src="images/projects/ECCV 2020 VO.mp4">
<source src="images/projects/ECCV 2020 VO.mp4" type="video/mp4">
</video></td><td align="left">
<p><b>Learning Monocular Visual Odometry via Self-Supervised Long-Term Modeling</b> <br />
<a href="https://yuliang.vision/">Yuliang Zou</a>,
<a href="https://sites.google.com/site/peterji1990">Pan Ji</a>,
<a href="https://cs.adelaide.edu.au/~huy/home.php">Quoc-Huy Tran</a>,
<a href="http://www.jiabinhuang.com">Jia-Bin Huang</a>, and
<a href="https://cseweb.ucsd.edu/~mkchandraker/">Manmohan Chandraker</a>
<br />
<i>Proceedings of European Conference on Computer Vision (ECCV), 2020</i> <br />
[<a href="https://arxiv.org/abs/2007.10983">Paper (PDF)</a>]
[<a href="https://yuliang.vision/LTMVO/">Project page</a>]
[<a href="">Code</a>]
[<a href="">Colab notebook</a>] <br /></p>
<p>
<a class="github-button" href="" data-icon="octicon-star" data-size="large" data-show-count="true" aria-label="Star vt-vl-lab/3d-photo-inpainting on GitHub">Star</a>
<a class="github-button" href="" data-icon="octicon-repo-forked" data-size="large" data-show-count="true" aria-label="Fork vt-vl-lab/3d-photo-inpainting on GitHub">Fork</a>
</p>
</td></tr></table>
<table class="imgtable"><tr><td>
<video id="vid" muted="muted" width="400" controls= "autoPlay" onended="this.currentTime = 0; this.play();" autoplay="" src="images/projects/SIGGRAPH 2020 ConsistentVideoDepth.mp4"> <source src="images/projects/SIGGRAPH 2020 ConsistentVideoDepth.mp4" type="video/mp4">
</video></td><td align="left">
<p><b>Consistent Video Depth Estimation</b> <br />
<a href="https://roxanneluo.github.io/">Xuan Luo</a>,
<a href="http://www.jiabinhuang.com">Jia-Bin Huang</a>,
<a href="http://szeliski.org/RichardSzeliski.htm">Richard Szeliski</a>,
<a href="https://www.linkedin.com/in/kevin-matzen-b3714414/">Kevin Matzen</a>, and
<a href="https://johanneskopf.de/">Johannes Kopf</a><br />
<i>ACM Transactions on Graphics (Proceedings of SIGGRAPH), 2020</i> <br />
[<a href="https://arxiv.org/abs/2004.15021">Paper (PDF)</a>]
[<a href="https://roxanneluo.github.io/Consistent-Video-Depth-Estimation/">Project page</a>]
[<a href="https://www.youtube.com/watch?v=5Tia2oblJAg">Video</a>]
[<a href="https://github.com/facebookresearch/consistent_depth">Code</a>]
[<a href="https://colab.research.google.com/drive/1i5_uVHWOJlh2adRFT5BuDhoRftq9Oosx">Colab notebook</a>]
[<a href="https://www.youtube.com/watch?v=QSVrKK_uHoU">Two minute video</a>] <br /></p>
<p>
<a class="github-button" href="https://github.com/facebookresearch/consistent_depth" data-icon="octicon-star" data-size="large" data-show-count="true" aria-label="Star ffacebookresearch/consistent_depth on GitHub">Star</a>
<a class="github-button" href="https://github.com/facebookresearch/consistent_depth/fork" data-icon="octicon-repo-forked" data-size="large" data-show-count="true" aria-label="Fork facebookresearch/consistent_depth on GitHub">Fork</a>
</p>
</td></tr></table>
<table class="imgtable"><tr><td>
<video id="vid" muted="muted" width="400" controls= "autoPlay" onended="this.currentTime = 0; this.play();" autoplay="" src="images/projects/CVPR 2020 3DPhoto.mp4"> <source src="images/projects/CVPR 2020 3DPhoto.mp4" type="video/mp4">
</video></td><td align="left">
<p><b>3D Photography using Context-aware Layered Depth Inpainting</b> <br />
<a href="http://shihmengli.github.io/">Meng-Li Shih</a>,
<a href="https://lemonatsu.github.io/home">Shih-Yang Su</a>,
<a href="https://johanneskopf.de/">Johannes Kopf</a>, and
<a href="http://www.jiabinhuang.com">Jia-Bin Huang</a> <br />
<i>Proceedings of IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2020</i> <br />
[<a href="https://filebox.ece.vt.edu/~jbhuang/project/3DPhoto/3DPhoto_paper.pdf">Paper (PDF)</a>]
[<a href="https://shihmengli.github.io/3D-Photo-Inpainting/">Project page</a>]
[<a href="https://github.com/vt-vl-lab/3d-photo-inpainting/">Code</a>]
[<a href="https://colab.research.google.com/drive/1706ToQrkIZshRSJSHvZ1RuCiM__YX3Bz">Colab notebook</a>]
[<a href="https://www.youtube.com/watch?v=MrIbQ0pIFOg">Two minute video</a>] <br />
News coverage: [<a href="https://syncedreview.com/2020/04/13/ai-transforms-rgb-d-images-into-an-impressive-3d-format/">Synced</a>] [<a href="https://gigazine.net/news/20200417-3d-photography-layered-depth-inpainting/">Gigazine</a>]
[<a href="https://www.digitalinformationworld.com/2020/05/facebook-ai-experts-claims-they-have-developed-a-better-way-to-create-3d-images.html">Digital Information World</a>]</p>
<p>
<a class="github-button" href="https://github.com/vt-vl-lab/3d-photo-inpainting" data-icon="octicon-star" data-size="large" data-show-count="true" aria-label="Star vt-vl-lab/3d-photo-inpainting on GitHub">Star</a>
<a class="github-button" href="https://github.com/vt-vl-lab/3d-photo-inpainting/fork" data-icon="octicon-repo-forked" data-size="large" data-show-count="true" aria-label="Fork vt-vl-lab/3d-photo-inpainting on GitHub">Fork</a>
</p>
</td></tr></table>
<table class="imgtable"><tr><td>
<video id="vid" width="400" controls= "autoPlay" onended="this.currentTime = 0; this.play();" autoplay="" src="images/projects/CVPR 2020 Obstruction.mp4"> <source src="images/projects/CVPR 2020 Obstruction.mp4" type="video/mp4">
</video></td><td align="left">
<p><b>Learning to See Through Obstructions</b> <br />
<a href="http://www.cmlab.csie.ntu.edu.tw/~yulunliu/">Yu-Lun Liu</a>,
<a href="https://www.wslai.net/">Wei-Sheng Lai</a>,
<a href="https://faculty.ucmerced.edu/mhyang/">Ming-Hsuan Yang</a>,
<a href="https://www.csie.ntu.edu.tw/~cyy/">Yung-Yu Chuang</a>, and
<a href="http://www.jiabinhuang.com">Jia-Bin Huang</a><br />
<i>Proceedings of IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2020</i> <br />
[<a href="https://arxiv.org/pdf/2004.01180.pdf">Paper (PDF)</a>]
[<a href="https://alex04072000.github.io/ObstructionRemoval/">Project page</a>]
[<a href="https://github.com/alex04072000/ObstructionRemoval">Code</a>]
[<a href="https://colab.research.google.com/drive/1iOKknc0dePekUH2TEh28EhcRPCS1mgwz">Colab notebook</a>]
[<a href="https://www.youtube.com/watch?v=ICr6xi9wA94">Two minute video</a>] <br />
News coverage: [<a href="https://www.newscientist.com/article/2253195-ai-removes-unwanted-objects-from-photos-to-give-a-clearer-view/">New Scientists</a>]</p>
<p>
<a class="github-button" href="https://github.com/alex04072000/ObstructionRemoval" data-icon="octicon-star" data-size="large" data-show-count="true" aria-label="Star alex04072000/ObstructionRemoval on GitHub">Star</a>
<a class="github-button" href="https://github.com/alex04072000/ObstructionRemoval/fork" data-icon="octicon-repo-forked" data-size="large" data-show-count="true" aria-label="Fork alex04072000/ObstructionRemoval on GitHub">Fork</a>
</p>
</td></tr></table>
<table class="imgtable"><tr><td>
<video id="vid" width="400" controls= "autoPlay" onended="this.currentTime = 0; this.play();" autoplay="" src="images/projects/CVPR 2020 HDR.mp4"> <source src="images/projects/CVPR 2020 HDR.mp4" type="video/mp4">
</video></td><td align="left">
<p><b>Single-Image HDR Reconstruction by Learning to Reverse the Camera Pipeline</b> <br />
<a href="http://www.cmlab.csie.ntu.edu.tw/~yulunliu/">Yu-Lun Liu</a>*,
<a href="https://www.wslai.net/">Wei-Sheng Lai</a>*,
<a href="">Yu-Sheng Chen</a>,
<a href="">Yi-Lung Kao</a>,
<a href="https://faculty.ucmerced.edu/mhyang/">Ming-Hsuan Yang</a>,
<a href="https://www.csie.ntu.edu.tw/~cyy/">Yung-Yu Chuang</a>, and
<a href="http://www.jiabinhuang.com">Jia-Bin Huang</a><br />
<i>Proceedings of IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2020</i> <br />
[<a href="https://arxiv.org/pdf/2004.01179">Paper (PDF)</a>]
[<a href="https://alex04072000.github.io/SingleHDR/">Project page</a>]
[<a href="https://github.com/alex04072000/SingleHDR">Code</a>]
[<a href="https://colab.research.google.com/drive/1WzNaGSaucF2AMDSdUCBMEOauBg4IowMa">Colab notebook</a>]</p>
<p>
<a class="github-button" href="https://github.com/alex04072000/SingleHDR" data-icon="octicon-star" data-size="large" data-show-count="true" aria-label="Star alex04072000/SingleHDR on GitHub">Star</a>
<a class="github-button" href="https://github.com/alex04072000/SingleHDR/fork" data-icon="octicon-repo-forked" data-size="large" data-show-count="true" aria-label="Fork alex04072000/SingleHDR on GitHub">Fork</a>
</p>
</td></tr></table>
<table class="imgtable"><tr><td>
<video id="vid" width="400" controls= "autoPlay" onended="this.currentTime = 0; this.play();" autoplay="" src="images/projects/CVPR 2020 Colorization.mp4"> <source src="images/projects/CVPR 2020 Colorization.mp4" type="video/mp4">
</video></td><td align="left">
<p><b>Instance-aware Image Colorization</b> <br />
<a href="">Jheng-Wei Su</a>,
<a href="http://cgv.cs.nthu.edu.tw/hkchu/">Hung-Kuo Chu</a>, and
<a href="http://www.jiabinhuang.com">Jia-Bin Huang</a> <br />
<i>Proceedings of IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2020</i> <br />
[<a href="https://arxiv.org/abs/2005.10825">Paper (PDF)</a>]
[<a href="https://ericsujw.github.io/InstColorization/">Project page</a>]
[<a href="https://github.com/ericsujw/InstColorization">Code</a>]
[<a href="https://colab.research.google.com/github/ericsujw/InstColorization/blob/master/InstColorization.ipynb">Colab notebook</a>]
[<a href="https://www.youtube.com/watch?v=Zj1N4uE1ehk">Two minute video</a>] <br /></p>
<p>
<a class="github-button" href="https://github.com/ericsujw/InstColorization" data-icon="octicon-star" data-size="large" data-show-count="true" aria-label="Star ericsujw/InstColorization on GitHub">Star</a>
<a class="github-button" href="https://github.com/ericsujw/InstColorization/fork" data-icon="octicon-repo-forked" data-size="large" data-show-count="true" aria-label="Fork ericsujw/InstColorization on GitHub">Fork</a>
</p>
</td></tr></table>
<table class="imgtable"><tr><td>
<img src="images/projects/ICLR 2020 CrossDomainFewShot.jpg" alt="" width="400px" />&nbsp;</td>
<td align="left"><p><b>Cross-Domain Few-Shot Classification via Learned Feature-Wise Transformation</b> <br />
<a href="https://sites.google.com/site/hytseng0509/">Hung-Yu Tseng</a>,
<a href="http://vllab.ucmerced.edu/hylee/">Hsin-Ying Lee</a>,
<a href="http://www.jiabinhuang.com">Jia-Bin Huang</a>, and
<a href="http://faculty.ucmerced.edu/mhyang/">Ming-Hsuan Yang</a><br />
<i>Proceedings of International Conference on Learning Representations (ICLR), 2020</i> (Spotlight Oral Presentation)  <br />
[<a href="https://openreview.net/forum?id=SJl5Np4tPr">Paper (PDF)</a>]
[<a href="http://vllab.ucmerced.edu/ym41608/projects/CrossDomainFewShot">Project page</a>]
[<a href="https://github.com/hytseng0509/CrossDomainFewShot">Code</a>]</p>
<p>
<a class="github-button" href="https://github.com/hytseng0509/CrossDomainFewShot" data-icon="octicon-star" data-size="large" data-show-count="true" aria-label="Star hytseng0509/CrossDomainFewShot on GitHub">Star</a>
<a class="github-button" href="https://github.com/hytseng0509/CrossDomainFewShot/fork" data-icon="octicon-repo-forked" data-size="large" data-show-count="true" aria-label="Fork hytseng0509/CrossDomainFewShot on GitHub">Fork</a>
</p>
</td></tr></table>
<table class="imgtable"><tr><td>
<video id="vid" width="400" controls= "autoPlay" onended="this.currentTime = 0; this.play();" autoplay="" src="images/projects/WACV 2020 Drone.mp4"> <source src="images/projects/WACV 2020 Drone.mp4" type="video/mp4">
</video></td><td align="left">
<p><b>Unsupervised and Semi-Supervised Domain Adaptation for Action Recognition from Drones</b> <br />
<a href="https://sites.google.com/site/jchoivision/">Jinwoo Choi</a>,
<a href="http://www.grvsharma.com/research.html">Gaurav Sharma</a>,
<a href="https://cseweb.ucsd.edu/~mkchandraker/">Manmohan Chandraker</a>, and
<a href="http://www.jiabinhuang.com">Jia-Bin Huang</a> <br />
<i>Proceedings of Winter Conference on Applications of Computer Vision (WACV), 2020</i> <br />
[<a href="http://openaccess.thecvf.com/content_WACV_2020/papers/Choi_Unsupervised_and_Semi-Supervised_Domain_Adaptation_for_Action_Recognition_from_Drones_WACV_2020_paper.pdf">Paper (PDF)</a>]
[<a href="https://github.com/jinwchoi/NEC-Drone-Dataset">Dataset</a>]
[<a href="supp/WACV_2020_Drone_Poster.pdf">Poster</a>]
[<a href="supp/WACV_2020_Drone_Slides.pptx">Slides</a>]</p>
<p>
<a class="github-button" href="https://github.com/jinwchoi/NEC-Drone-Dataset" data-icon="octicon-star" data-size="large" data-show-count="true" aria-label="Star jinwchoi/NEC-Drone-Dataset on GitHub">Star</a>
<a class="github-button" href="https://github.com/jinwchoi/NEC-Drone-Dataset/fork" data-icon="octicon-repo-forked" data-size="large" data-show-count="true" aria-label="Fork jinwchoi/NEC-Drone-Dataset on GitHub">Fork</a>
</p>
</td></tr></table>
<table class="imgtable"><tr><td>
<img src="images/projects/WACV 2020 Footskate.jpg" alt="" width="400px" />&nbsp;</td>
<td align="left"><p><b>Reducing Footskate in Human Motion Reconstruction with Ground Contact Constraints</b> <br />
<a href="https://yuliang.vision/">Yuliang Zou</a>,
<a href="https://eng.ucmerced.edu/people/jyang44">Jimei Yang</a>,
<a href="http://www.duygu-ceylan.com/">Duygu Ceylan</a>,
<a href="https://jimmie33.github.io/">Jianming Zhang</a>,
<a href="https://fperazzi.github.io/">Federico Perazzi</a>, and
<a href="http://www.jiabinhuang.com">Jia-Bin Huang</a> <br />
<i>Proceedings of Winter Conference on Applications of Computer Vision (WACV), 2020</i> <br />
[<a href="http://openaccess.thecvf.com/content_WACV_2020/papers/Zou_Reducing_Footskate_in_Human_Motion_Reconstruction_with_Ground_Contact_Constraints_WACV_2020_paper.pdf">Paper (PDF)</a>]
[<a href="https://github.com/vt-vl-lab">Project page</a>]
[<a href="https://github.com/vt-vl-lab/footskate_reducer">Code</a>]
[<a href="supp/WACV_2020_Footskate_Poster.pdf">Poster</a>]
[<a href="supp/WACV_2020_Footskate_Slides.pptx">Slides</a>]</p>
<p>
<a class="github-button" href="https://github.com/vt-vl-lab/footskate_reducer" data-icon="octicon-star" data-size="large" data-show-count="true" aria-label="Star vt-vl-lab/footskate_reducer on GitHub">Star</a>
<a class="github-button" href="https://github.com/vt-vl-lab/footskate_reducer/fork" data-icon="octicon-repo-forked" data-size="large" data-show-count="true" aria-label="Fork vt-vl-lab/footskate_reducer on GitHub">Fork</a>
</p>
</td></tr></table>
<table class="imgtable"><tr><td>
<img src="images/projects/arXiv 2019 MaCoS.jpg" alt="" width="400px" />&nbsp;</td>
<td align="left"><p><b>Show, Match and Segment: Joint Weakly Supervised Learning of Semantic Matching and Object Co-segmentation</b> <br />
<a href="https://yunchunchen.github.io/">Yun-Chun Chen</a>,
<a href="https://www.citi.sinica.edu.tw/pages/yylin/">Yen-Yu Lin</a>,
<a href="http://faculty.ucmerced.edu/mhyang/">Ming-Hsuan Yang</a>, and
<a href="http://www.jiabinhuang.com">Jia-Bin Huang</a> <br />
<i>IEEE Transactions on Pattern Recognition and Machine Intelligence (PAMI), 2020</i> <br />
[<a href="https://arxiv.org/abs/1906.05857">Paper (PDF)</a>]
[<a href="https://yunchunchen.github.io/MaCoSNet-web/">Project page</a>]
[<a href="https://github.com/YunChunChen/MaCoSNet-pytorch">Code</a>]</p>
<p>
<a class="github-button" href="https://github.com/YunChunChen/MaCoSNet-pytorch" data-icon="octicon-star" data-size="large" data-show-count="true" aria-label="Star YunChunChen/MaCoSNet-pytorch on GitHub">Star</a>
<a class="github-button" href="https://github.com/YunChunChen/MaCoSNet-pytorch/fork" data-icon="octicon-repo-forked" data-size="large" data-show-count="true" aria-label="Fork YunChunChen/MaCoSNet-pytorch on GitHub">Fork</a>
</p>
</td></tr></table>
<table class="imgtable"><tr><td>
<img src="images/projects/IJCV 2020 DRIT++.jpg" alt="" width="400px" />&nbsp;</td>
<td align="left"><p><b>DRIT++: Diverse Image-to-Image Translation via Disentangled Representations</b> <br />
<a href="http://vllab.ucmerced.edu/hylee/">Hsin-Ying Lee</a>*,
<a href="https://sites.google.com/site/hytseng0509/">Hung-Yu Tseng</a>*,
<a href="https://sites.google.com/view/qi-mao/">Qi Mao</a>*
<a href="http://www.jiabinhuang.com">Jia-Bin Huang</a>,
<a href="https://jonlu0602.github.io/">Yu-Ding Lu</a>,
<a href="https://www.linkedin.com/in/maneesh-singh-3523ab9">Maneesh Singh</a>,
<a href="http://faculty.ucmerced.edu/mhyang/">Ming-Hsuan Yang</a> <br />
<i>International Journal of Computer Vision (IJCV), 2020</i> <br />
[<a href="papers/DRIT_pp.pdf">Paper (PDF)</a>]
[<a href="http://vllab.ucmerced.edu/hylee/DRIT_pp/">Project page</a>]
[<a href="https://github.com/HsinYingLee/DRIT">Code</a>]</p>
<p>
<a class="github-button" href="https://github.com/HsinYingLee/DRIT" data-icon="octicon-star" data-size="large" data-show-count="true" aria-label="Star HsinYingLee/DRIT on GitHub">Star</a>
<a class="github-button" href="https://github.com/HsinYingLee/DRIT/fork" data-icon="octicon-repo-forked" data-size="large" data-show-count="true" aria-label="Fork HsinYingLee/DRIT on GitHub">Fork</a>
</p>
</td></tr></table>
<table class="imgtable"><tr><td>
<img src="images/projects/ECCV 2016 Face Tracking.jpg" alt="" width="400px" />&nbsp;</td>
<td align="left"><p><b>Tracking Persons-of-Interest via Unsupervised Representation Adaptation</b> <br />
<a href="https://sites.google.com/site/shunzhang876/">Shun Zhang</a>,
<a href="http://www.jiabinhuang.com">Jia-Bin Huang</a>,
<a href="http://cvlab.hanyang.ac.kr/~jwlim/">Jongwoo Lim</a>,
<a href="https://scholar.google.com/citations?user=x2xdU7gAAAAJ&amp;hl=en">Yihong Gong</a>,
<a href="http://xypeng.gr.xjtu.edu.cn/web/jinjun/english">Jinjun Wang</a>,
<a href="http://vision.ai.illinois.edu/ahuja.html">Narendra Ahuja</a>, and
<a href="http://faculty.ucmerced.edu/mhyang/">Ming-Hsuan Yang</a> <br />
<i>International Journal of Computer Vision (IJCV), 2020</i> <br />
[<a href="https://arxiv.org/abs/1710.02139">Paper (PDF)</a>]
[<a href="https://sites.google.com/site/shunzhang876/facetracking">Project page</a>]
[<a href="https://sites.google.com/site/shunzhang876/facetracking">Code</a>]</p>
<p>
<a class="github-button" href="https://github.com/shunzhang876/AdaptiveFeatureLearning " data-icon="octicon-star" data-size="large" data-show-count="true" aria-label="Star shunzhang876/AdaptiveFeatureLearning on GitHub">Star</a>
<a class="github-button" href="https://github.com/shunzhang876/AdaptiveFeatureLearning/fork" data-icon="octicon-repo-forked" data-size="large" data-show-count="true" aria-label="Fork shunzhang876/AdaptiveFeatureLearning on GitHub">Fork</a>
</p>
</td></tr></table>
<table class="imgtable"><tr><td>
<img src="images/projects/NeurIPS 2019 Action.jpg" alt="" width="400px" />&nbsp;</td>
<td align="left"><p><b>Why Can't I Dance in the Mall? Learning to Mitigate Scene Bias in Action Recognition</b> <br />
<a href="https://sites.google.com/site/jchoivision/">Jinwoo Choi</a>,
<a href="http://chengao.vision/">Chen Gao</a>,
<a href="https://josephcmessou.weebly.com/">Joseph Messou</a>, and
<a href="http://www.jiabinhuang.com">Jia-Bin Huang</a><br />
<i>Proceedings of Neural Information Processing Systems (NeurIPS), 2019</i> <br />
[<a href="http://papers.nips.cc/paper/8372-why-cant-i-dance-in-the-mall-learning-to-mitigate-scene-bias-in-action-recognition.pdf">Paper (PDF)</a>]
[<a href="http://chengao.vision/SDN/">Project page</a>]
[<a href="https://github.com/vt-vl-lab/SDN/">Code</a>]
[<a href="supp/NeurIPS_2019_Action_Poster.pdf">Poster</a>]</p>
<p>
<a class="github-button" href="https://github.com/vt-vl-lab/SDN/" data-icon="octicon-star" data-size="large" data-show-count="true" aria-label="Star vt-vl-lab/SDN on GitHub">Star</a>
<a class="github-button" href="https://github.com/vt-vl-lab/SDN/fork" data-icon="octicon-repo-forked" data-size="large" data-show-count="true" aria-label="Fork vt-vl-lab/SDN on GitHub">Fork</a>
</p>
</td></tr></table>
<table class="imgtable"><tr><td>
<img src="images/projects/ICCV 2019 GuidedPix2Pix.jpg" alt="" width="400px" />&nbsp;</td>
<td align="left"><p><b>Guided Image-to-Image Translation with Bi-Directional Feature Transformation</b> <br />
<a href="https://badouralbahar.github.io/">Badour AlBahar</a> and
<a href="http://www.jiabinhuang.com">Jia-Bin Huang</a><br />
<i>Proceedings of IEEE International Conference on Computer Vision (ICCV), 2019</i> <br />
[<a href="https://arxiv.org/abs/1910.11328">Paper (PDF)</a>]
[<a href="https://filebox.ece.vt.edu/~Badour/guided_pix2pix.html">Project page</a>]
[<a href="https://github.com/vt-vl-lab/Guided-pix2pix">Code</a>]
[<a href="https://filebox.ece.vt.edu/~Badour/supp.html">Supp</a>]
[<a href="supp/ICCV_2019_GuidedPix2PiX_Poster.pdf">Poster</a>]</p>
<p>
<a class="github-button" href="https://github.com/vt-vl-lab/Guided-pix2pix" data-icon="octicon-star" data-size="large" data-show-count="true" aria-label="Star vt-vl-lab/Guided-pix2pix on GitHub">Star</a>
<a class="github-button" href="https://github.com/vt-vl-lab/Guided-pix2pix/fork" data-icon="octicon-repo-forked" data-size="large" data-show-count="true" aria-label="Fork vt-vl-lab/Guided-pix2pix on GitHub">Fork</a>
</p>
</td></tr></table>
<table class="imgtable"><tr><td>
<img src="images/projects/CVPR 2019 CrDoCo.jpg" alt="" width="400px" />&nbsp;</td>
<td align="left"><p><b>CrDoCo: Pixel-level Domain Transfer with Cross-Domain Consistency</b> <br />
<a href="https://yunchunchen.github.io/">Yun-Chun Chen</a>,
<a href="https://www.citi.sinica.edu.tw/pages/yylin/">Yen-Yu Lin</a>,
<a href="http://faculty.ucmerced.edu/mhyang/">Ming-Hsuan Yang</a>, and
<a href="http://www.jiabinhuang.com">Jia-Bin Huang</a> <br />
<i>Proceedings of IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2019</i> <br />
[<a href="papers/CVPR_2019_CrDoCo.pdf">Paper (PDF)</a>]
[<a href="https://yunchunchen.github.io/CrDoCo/">Project page</a>]
[<a href="https://github.com/YunChunChen/CrDoCo-pytorch">Code</a>]
[<a href="supp/CVPR_2019_CrDoCo_Poster.pdf">Poster</a>]</p>
<p>
<a class="github-button" href="https://github.com/YunChunChen/CrDoCo-pytorch" data-icon="octicon-star" data-size="large" data-show-count="true" aria-label="Star YunChunChen/CrDoCo-pytorch on GitHub">Star</a>
<a class="github-button" href="https://github.com/YunChunChen/CrDoCo-pytorch/fork" data-icon="octicon-repo-forked" data-size="large" data-show-count="true" aria-label="Fork YunChunChen/CrDoCo-pytorch on GitHub">Fork</a>
</p>
</td></tr></table>
<table class="imgtable"><tr><td>
<video id="vid" width="400" controls= "autoPlay" onended="this.currentTime = 0; this.play();" autoplay="" src="images/projects/CVPR 2019 Amodal.mp4"> <source src="images/projects/CVPR 2019 Amodal.mp4" type="video/mp4">
</video></td><td align="left">
<p><b>SAIL-VOS: Semantic Amodal Instance Level Video Object Segmentation - A Synthetic Dataset and Baselines</b> <br />
<a href="https://sites.google.com/view/yuantinghu">Yuan-Ting Hu</a>,
<a href="">Hong-Shuo Chen</a>,
<a href="">Kexin Hui</a>,
<a href="http://www.jiabinhuang.com">Jia-Bin Huang</a>, and
<a href="http://www.alexander-schwing.de/">Alexander Schwing</a>  <br />
<i>Proceedings of IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2019</i> <br />
[<a href="http://openaccess.thecvf.com/content_CVPR_2019/papers/Hu_SAIL-VOS_Semantic_Amodal_Instance_Level_Video_Object_Segmentation_-_A_CVPR_2019_paper.pdf">Paper (PDF)</a>]
[<a href="http://sailvos.web.illinois.edu/_site/index.html">Project page</a>]
[<a href="http://sailvos.web.illinois.edu/_site/index.html">Code</a>]
[<a href="supp/CVPR_2019_AmodalSeg_Poster.pdf">Poster</a>]</p>
</td></tr></table>
<table class="imgtable"><tr><td>
<img src="images/projects/ICLR 2019 Few-shot.jpg" alt="" width="400px" />&nbsp;</td>
<td align="left"><p><b>A Closer Look at Few-shot Classification</b> <br />
<a href="https://wyharveychen.github.io/">Wei-Yu Chen</a>,
<a href="https://ycliu93.github.io/">Yen-Cheng Liu</a>,
<a href="https://www.cc.gatech.edu/~zk15/">Zsolt Kira</a>,
<a href="http://vllab.ee.ntu.edu.tw/members.html">Yu-Chiang Frank Wang</a>, and
<a href="http://www.jiabinhuang.com">Jia-Bin Huang</a> <br />
<i>International Conference on Learning Representation (ICLR) 2019</i> <br />
[<a href="https://openreview.net/forum?id=HkxLXnAcFQ">Paper (PDF)</a>]
[<a href="https://sites.google.com/view/a-closer-look-at-few-shot/">Project page</a>]
[<a href="https://github.com/wyharveychen/CloserLookFewShot">Code</a>]
[<a href="supp/ICLR_2019_FewShotLearning_Poster.pptx">Poster</a>]</p>
<p>
<a class="github-button" href="https://github.com/wyharveychen/CloserLookFewShot" data-icon="octicon-star" data-size="large" data-show-count="true" aria-label="Star wyharveychen/CloserLookFewShot on GitHub">Star</a>
<a class="github-button" href="https://github.com/wyharveychen/CloserLookFewShot/fork" data-icon="octicon-repo-forked" data-size="large" data-show-count="true" aria-label="Fork wyharveychen/CloserLookFewShot on GitHub">Fork</a>
</p>
</td></tr></table>
<table class="imgtable"><tr><td>
<img src="images/projects/AAAI 2019 Attack.jpg" alt="" width="400px" />&nbsp;</td>
<td align="left"><p><b>Connecting the Digital and Physical World: Improving the Robustness of Adversarial Attacks</b> <br />
<a href="https://stevetkjan.github.io/">Steve T. K. Jan</a>,
<a href="https://josephcmessou.weebly.com/">Joseph Messou</a>,
<a href="http://yclin.me/">Yen-Chen Lin</a>,
<a href="http://www.jiabinhuang.com">Jia-Bin Huang</a>, and
<a href="http://people.cs.vt.edu/~gangwang/">Gang Wang</a> <br />
<i>Proceedings of AAAI Conference on Artificial Intelligence (AAAI) 2019</i> <br />
[<a href="http://people.cs.vt.edu/gangwang/AAAI19.pdf">Paper (PDF)</a>]
[<a href="http://people.cs.vt.edu/tekang/D2P/">Project page</a>]
[<a href="https://github.com/stevetkjan/Digital2Physical">Code</a>]</p>
<p>
<a class="github-button" href="https://github.com/stevetkjan/Digital2Physical" data-icon="octicon-star" data-size="large" data-show-count="true" aria-label="Star stevetkjan/Digital2Physical on GitHub">Star</a>
<a class="github-button" href="https://github.com/stevetkjan/Digital2Physical/fork" data-icon="octicon-repo-forked" data-size="large" data-show-count="true" aria-label="Fork stevetkjan/Digital2Physical on GitHub">Fork</a>
</p>
</td></tr></table>
<table class="imgtable"><tr><td>
<img src="images/projects/arXiv 2018 Paper Gestalt.jpg" alt="" width="400px" />&nbsp;</td>
<td align="left"><p><b>Deep Paper Gestalt</b> <br />
<a href="http://www.jiabinhuang.com">Jia-Bin Huang</a> <br />
<i>arXiv:1812.08775, 2018</i> <br />
<i>Secret Proceedings of Computer Vision and Pattern Recognition (s-CVPR), 2019</i> <br />
[<a href="https://arxiv.org/pdf/1812.08775.pdf">Paper (PDF)</a>]
[<a href="https://github.com/vt-vl-lab/paper-gestalt">Project page</a>]
[<a href="https://github.com/vt-vl-lab/paper-gestalt">Code</a>] <br />
News coverage:
[<a href="https://syncedreview.com/2018/12/27/can-ai-judge-a-paper-on-appearance-alone/">Syncreview</a>]
[<a href="https://www.zdnet.com/article/should-ai-researchers-trust-ai-to-vet-their-work/">ZDNet</a>]</p>
<p>
<a class="github-button" href="https://github.com/vt-vl-lab/paper-gestalt" data-icon="octicon-star" data-size="large" data-show-count="true" aria-label="Star vt-vl-lab/paper-gestalt on GitHub">Star</a>
<a class="github-button" href="https://github.com/vt-vl-lab/paper-gestalt/fork" data-icon="octicon-repo-forked" data-size="large" data-show-count="true" aria-label="Fork vt-vl-lab/paper-gestalt on GitHub">Fork</a>

<br /></p>
</td></tr></table>
<table class="imgtable"><tr><td>
<img src="images/projects/Siggraph 2019 studio source form.jpg" alt="" width="400px" />&nbsp;</td>
<td align="left"><p><b>Source Form: An Automated Crowdsourced Object Generator</b> <br />
<a href="https://www.sova.vt.edu/faculty/sam-blanchard/">Sam Blanchard</a>,
<a href="www.jiabinhuang.com">Jia-Bin Huang</a>,
<a href="http://www.me.vt.edu/people/faculty/christopher-bryant-williams/">Christopher B. Williams</a>,
<a href="http://seb199.me.vt.edu/dreams/viswanath-meenakshisundaram/">Viswanath Meenakshisundaram</a>,
<a href="http://seb199.me.vt.edu/dreams/joseph-kubalak/">Joseph Kubalak</a>, and
<a href="https://sanketloke.github.io/">Sanket Lokegaonkar</a> <br />
<i>ACM SIGGRAPH Studio 2019</i> <br />
[<a href="papers/Siggraph_2019_source_form.pdf">Paper (PDF)</a>]
[<a href="https://dl.acm.org/ft_gateway.cfm?id=3328001&amp;ftid=2074868&amp;dwn=1&amp;CFID=154826000&amp;CFTOKEN=ae06f4ebf54a827-4BC89191-D3A6-5D23-51B549EEFD97801A">Video</a>] <br />
News coverage:
[<a href="https://3dprintingindustry.com/news/virginia-techs-source-form-method-eliminates-need-for-3d-printer-stls-159965/">3D Printing Industry</a>]
[<a href="https://wherrelz.com">Wherrelz</a>]</p>
</td></tr></table>
<table class="imgtable"><tr><td>
<img src="images/projects/CVPR 2016 WSL.jpg" alt="" width="400px" />&nbsp;</td>
<td align="left"><p><b>Progressive Representation Adaptation for Weakly Supervised Object Localization</b> <br />
<a href="https://sites.google.com/site/lidonggg930">Dong Li</a>,
<a href="http://www.jiabinhuang.com">Jia-Bin Huang</a>,
<a href="http://dblp.uni-trier.de/pers/hd/l/Li:Yali">Yali Li</a>,
<a href="http://www.tsinghua.edu.cn/publish/eeen/3784/2010/20101219115601212198627/20101219115601212198627_.html">Shengjin Wang</a>, and
<a href="http://faculty.ucmerced.edu/mhyang/">Ming-Hsuan Yang</a> <br />
<i>IEEE Transactions on Pattern Analysis and Machine Intelligence (PAMI), 2019</i> <br />
[<a href="https://arxiv.org/abs/1710.04647">Paper (PDF)</a>]
[<a href="https://sites.google.com/site/lidonggg930/wsl">Project page</a>]
[<a href="https://github.com/jbhuang0604/WSL/">Code</a>]<br /></p>
<p>
<a class="github-button" href="https://github.com/jbhuang0604/WSL/" data-icon="octicon-star" data-size="large" data-show-count="true" aria-label="Star jbhuang0604/WSL on GitHub">Star</a>
<a class="github-button" href="https://github.com/jbhuang0604/WSL/fork" data-icon="octicon-repo-forked" data-size="large" data-show-count="true" aria-label="Fork jbhuang0604/WSL on GitHub">Fork</a>
</p>
</td></tr></table>
<table class="imgtable"><tr><td>
<img src="images/projects/PAMI17 Deep Joint Image Filter.jpg" alt="" width="400px" />&nbsp;</td>
<td align="left"><p><b>Joint Image Filtering with Deep Convolutional Networks</b> <br />
<a href="https://sites.google.com/site/yijunlimaverick/">Yijun Li</a>,
<a href="http://www.jiabinhuang.com">Jia-Bin Huang</a>,
<a href="http://vision.ai.illinois.edu/ahuja.html">Narendra Ahuja</a>, and
<a href="http://faculty.ucmerced.edu/mhyang/">Ming-Hsuan Yang</a> <br />
<i>IEEE Transactions on Pattern Analysis and Machine Intelligence (PAMI), 2019</i> <br />
[<a href="https://arxiv.org/abs/1710.04200">Paper (PDF)</a>]
[<a href="http://vllab1.ucmerced.edu/~yli62/DJF_residual/">Project page</a>]
[<a href="https://github.com/Yijunmaverick/DeepJointFilter">Code</a>]</p>
<p>
<a class="github-button" href="https://github.com/Yijunmaverick/DeepJointFilter" data-icon="octicon-star" data-size="large" data-show-count="true" aria-label="Star Yijunmaverick/DeepJointFilter on GitHub">Star</a>
<a class="github-button" href="https://github.com/Yijunmaverick/DeepJointFilter/fork" data-icon="octicon-repo-forked" data-size="large" data-show-count="true" aria-label="Fork Yijunmaverick/DeepJointFilter on GitHub">Fork</a>
</p>
</td></tr></table>
<table class="imgtable"><tr><td>
<video id="vid" width="400" controls= "autoPlay" onended="this.currentTime = 0; this.play();" autoplay="" src="images/projects/SigAsia 2018 WireArt.mp4"> <source src="images/projects/SigAsia 2018 WireArt.mp4" type="video/mp4">
</video></td><td align="left">
<p><b>Multi-view Wire Art</b> <br />
<a href="">Kai-Wen Hsiao</a>,
<a href="http://www.jiabinhuang.com">Jia-Bin Huang</a>, and
<a href="http://cgv.cs.nthu.edu.tw/hkchu/">Hung-Kuo Chu</a> <br />
<i>ACM Transactions on Graphics (Proceedings of SIGGRAPH Asia), 2018</i> (Oral Presentation) <br />
[<a href="https://cgv.cs.nthu.edu.tw/download/file?guid=3d4d4ee9-cdec-11e8-9b71-0011328fa92e">Paper (PDF)</a>]
[<a href="https://cgv.cs.nthu.edu.tw/projects/recreational_graphics/MVWA">Project page</a>]
[<a href="https://github.com/KaiWenHsiao/Multi-view-Wire-Art">Code</a>]</p>
<p>
<a class="github-button" href="https://github.com/KaiWenHsiao/Multi-view-Wire-Art" data-icon="octicon-star" data-size="large" data-show-count="true" aria-label="Star KaiWenHsiao/Multi-view-Wire-Art on GitHub">Star</a>
<a class="github-button" href="https://github.com/KaiWenHsiao/Multi-view-Wire-Art/fork" data-icon="octicon-repo-forked" data-size="large" data-show-count="true" aria-label="Fork KaiWenHsiao/Multi-view-Wire-Art on GitHub">Fork</a>
</p>
</td></tr></table>
<table class="imgtable"><tr><td>
<img src="images/projects/ECCV 2018 DRIT.gif" alt="" width="400px" />&nbsp;</td>
<td align="left"><p><b>Diverse Image-to-Image Translation via Disentangled Representations</b> <br />
<a href="http://vllab.ucmerced.edu/hylee/">Hsin-Ying Lee</a>*,
<a href="https://sites.google.com/site/hytseng0509/">Hung-Yu Tseng</a>*,
<a href="http://www.jiabinhuang.com">Jia-Bin Huang</a>,
<a href="https://www.linkedin.com/in/maneesh-singh-3523ab9">Maneesh Singh</a>,
<a href="http://faculty.ucmerced.edu/mhyang/">Ming-Hsuan Yang</a> <br />
<i>Proceedings of European Conference on Computer Vision (ECCV), 2018</i> (Oral Presentation) <br />
[<a href="https://arxiv.org/pdf/1808.00948.pdf">Paper (PDF)</a>]
[<a href="http://vllab.ucmerced.edu/hylee/DRIT/">Project page</a>]
[<a href="https://github.com/HsinYingLee/DRIT">Code</a>]
[<a href="supp/ECCV_2018_DRIT_Slides.pptx">Slides</a>]
[<a href="supp/ECCV_2018_DRIT_Poster.pdf">Poster</a>]</p>
<p>
<a class="github-button" href="https://github.com/HsinYingLee/DRIT" data-icon="octicon-star" data-size="large" data-show-count="true" aria-label="Star HsinYingLee/DRIT on GitHub">Star</a>
<a class="github-button" href="https://github.com/HsinYingLee/DRIT/fork" data-icon="octicon-repo-forked" data-size="large" data-show-count="true" aria-label="Fork HsinYingLee/DRIT on GitHub">Fork</a>
</p>
</td></tr></table>
<table class="imgtable"><tr><td>
<img src="images/projects/ECCV 2018 Consistency.gif" alt="" width="400px" />&nbsp;</td>
<td align="left"><p><b>DF-Net: Unsupervised Joint Learning of Depth and Flow using Cross-Task Consistency</b> <br />
<a href="http://yuliang.vision/">Yuliang Zou</a>,
<a href="http://alan.vision/">Zelun Luo</a>, and
<a href="http://www.jiabinhuang.com">Jia-Bin Huang</a> <br />
<i>Proceedings of European Conference on Computer Vision (ECCV), 2018</i> <br />
[<a href="https://arxiv.org/abs/1809.01649">Paper (PDF)</a>]
[<a href="http://yuliang.vision/DF-Net/">Project page</a>]
[<a href="https://github.com/vt-vl-lab/DF-Net">Code</a>]
[<a href="supp/ECCV_2018_DFNet_Poster.pdf">Poster</a>]</p>
<p>
<a class="github-button" href="https://github.com/vt-vl-lab/DF-Net" data-icon="octicon-star" data-size="large" data-show-count="true" aria-label="Star vt-vl-lab/DF-Net on GitHub">Star</a>
<a class="github-button" href="https://github.com/vt-vl-lab/DF-Net/fork" data-icon="octicon-repo-forked" data-size="large" data-show-count="true" aria-label="Fork vt-vl-lab/DF-Net on GitHub">Fork</a>
</p>
</td></tr></table>
<table class="imgtable"><tr><td>
<video id="vid" width="400" controls= "autoPlay" onended="this.currentTime = 0; this.play();" autoplay="" src="images/projects/ECCV 2018 TemporalConsistency.mp4">
<source src="images/projects/ECCV 2018 TemporalConsistency.mp4" type="video/mp4">
</video>
</td><td align="left">
<p><b>Learning Blind Video Temporal Consistency</b> <br />
<a href="http://graduatestudents.ucmerced.edu/wlai24/">Wei-Sheng Lai</a>,
<a href="http://www.jiabinhuang.com">Jia-Bin Huang</a>,
<a href="http://www.oliverwang.info/">Oliver Wang</a>,
<a href="https://research.adobe.com/person/eli-shechtman/">Eli Shechtman</a>,
<a href="http://www.meyumer.com/">Ersin Yumer</a>,
<a href="http://faculty.ucmerced.edu/mhyang/">Ming-Hsuan Yang</a> <br />
<i>Proceedings of European Conference on Computer Vision (ECCV), 2018</i> <br />
[<a href="https://arxiv.org/pdf/1808.00449.pdf">Paper (PDF)</a>]
[<a href="http://vllab.ucmerced.edu/wlai24/video_consistency/">Project page</a>]
[<a href="https://github.com/phoenix104104/fast_blind_video_consistency">Code</a>]
[<a href="supp/ECCV_2018_VideoConsistency_Poster.pdf">Poster</a>]
[<a href="supp/ECCV_2018_VideoConsistency_Supp.pdf">Supp</a>]</p>
<p>
<a class="github-button" href="https://github.com/phoenix104104/fast_blind_video_consistency" data-icon="octicon-star" data-size="large" data-show-count="true" aria-label="Star phoenix104104/fast_blind_video_consistency on GitHub">Star</a>
<a class="github-button" href="https://github.com/phoenix104104/fast_blind_video_consistency/fork" data-icon="octicon-repo-forked" data-size="large" data-show-count="true" aria-label="Fork phoenix104104/fast_blind_video_consistency on GitHub">Fork</a>
</p>
</td></tr></table>
<table class="imgtable"><tr><td>
<video id="vid" width="400" controls= "autoPlay" onended="this.currentTime = 0; this.play();" autoplay="" src="images/projects/ECCV 2018 VideoMatch.mp4"> <source src="images/projects/ECCV 2018 VideoMatch.mp4" type="video/mp4">
</video></td><td align="left">
<p><b>VideoMatch: Matching based Video Object Segmentation</b> <br />
<a href="https://sites.google.com/view/yuantinghu">Yuan-Ting Hu</a>,
<a href="http://www.jiabinhuang.com">Jia-Bin Huang</a>, and
<a href="http://www.alexander-schwing.de/">Alexander Schwing</a>  <br />
<i>Proceedings of European Conference on Computer Vision (ECCV), 2018</i> <br />
[<a href="https://arxiv.org/abs/1809.01123">Paper (PDF)</a>]
[<a href="https://sites.google.com/view/videomatch">Project page</a>]
[<a href="supp/ECCV_2018_VideoMatch_Poster.pdf">Poster</a>]</p>
</td></tr></table>
<table class="imgtable"><tr><td>
<video id="vid" width="400" controls= "autoPlay" onended="this.currentTime = 0; this.play();" autoplay="" src="images/projects/ECCV 2018 ST-Prop.mp4"> <source src="images/projects/ECCV 2018 ST-Prop.mp4" type="video/mp4">
</video></td><td align="left">
<p><b>Unsupervised Video Object Segmentation using Motion Saliency-Guided Spatio-Temporal Propagation</b> <br />
<a href="https://sites.google.com/view/yuantinghu">Yuan-Ting Hu</a>,
<a href="http://www.jiabinhuang.com">Jia-Bin Huang</a>, and
<a href="http://www.alexander-schwing.de/">Alexander Schwing</a>  <br />
<i>Proceedings of European Conference on Computer Vision (ECCV), 2018</i> <br />
[<a href="https://arxiv.org/pdf/1809.01125.pdf">Paper (PDF)</a>]
[<a href="https://sites.google.com/view/unsupervisedvos">Project page</a>]
[<a href="supp/ECCV_2018_UnVideoSeg_Poster.pdf">Poster</a>]</p>
</td></tr></table>
<table class="imgtable"><tr><td>
<img src="images/projects/BMVC 2018 HOI.gif" alt="" width="400px" />&nbsp;</td>
<td align="left"><p><b>iCAN: Instance-Centric Attention Network for Human-Object Interaction Detection</b> <br />
<a href="https://gaochen315.github.io">Chen Gao</a>,
<a href="http://yuliang.vision/">Yuliang Zou</a>, and
<a href="http://www.jiabinhuang.com">Jia-Bin Huang</a> <br />
<i>Proceedings of British Machine Vision Conference (BMVC), 2018</i> <br />
[<a href="https://arxiv.org/abs/1808.10437">Paper (PDF)</a>]
[<a href="https://gaochen315.github.io/iCAN/">Project page</a>]
[<a href="https://github.com/vt-vl-lab/iCAN">Code</a>]
[<a href="supp/BMVC_2018_iCAN_Poster.pdf">Poster</a>]
[<a href="supp/BMVC_2018_iCAN_Supp.pdf">Supp</a>]</p>
<p>
<a class="github-button" href="https://github.com/vt-vl-lab/iCAN" data-icon="octicon-star" data-size="large" data-show-count="true" aria-label="Star vt-vl-lab/iCAN on GitHub">Star</a>
<a class="github-button" href="https://github.com/vt-vl-lab/iCAN/fork" data-icon="octicon-repo-forked" data-size="large" data-show-count="true" aria-label="Fork vt-vl-lab/iCAN on GitHub">Fork</a>
</p>
</td></tr></table>
<table class="imgtable"><tr><td>
<img src="images/projects/CVPR 2018 MVS.gif" alt="" width="400px" />&nbsp;</td>
<td align="left"><p><b>DeepMVS: Learning Multi-View Stereopsis</b> <br />
<a href="https://phuang17.github.io/">Po-Han Huang</a>,
<a href="http://www.kmatzen.com/">Kevin Matzen</a>,
<a href="http://johanneskopf.de/">Johannes Kopf</a>,
<a href="http://vision.ai.illinois.edu/ahuja.html">Narendra Ahuja</a>, and
<a href="http://www.jiabinhuang.com">Jia-Bin Huang</a> <br />
<i>Proceedings of IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2018</i> <br />
[<a href="https://arxiv.org/pdf/1804.00650.pdf">Paper (PDF)</a>]
[<a href="https://phuang17.github.io/DeepMVS/">Project page</a>]
[<a href="https://github.com/phuang17/DeepMVS">Code</a>]
[<a href="supp/CVPR_2018_DeepMVS_Poster.pdf">Poster</a>]
[<a href="supp/CVPR_2018_DeepMVS_Supp.pdf">Supp</a>]</p>
<p>
<a class="github-button" href="https://github.com/phuang17/DeepMVS" data-icon="octicon-star" data-size="large" data-show-count="true" aria-label="Star phuang17/DeepMVS on GitHub">Star</a>
<a class="github-button" href="https://github.com/phuang17/DeepMVS/fork" data-icon="octicon-repo-forked" data-size="large" data-show-count="true" aria-label="Fork phuang17/DeepMVS on GitHub">Fork</a>
</p>
</td></tr></table>
<table class="imgtable"><tr><td>
<img src="images/projects/ACCV 2018 Matching.jpg" alt="" width="400px" />&nbsp;</td>
<td align="left"><p><b>Deep Semantic Matching with Foreground Detection and Cycle-Consistency</b> <br />
<a href="https://yunchunchen.github.io/">Yun-Chun Chen</a>,
<a href="">Po-Hsiang Huang</a>,
<a href="">Li-Yu Yu</a>,
<a href="http://www.jiabinhuang.com">Jia-Bin Huang</a>,
<a href="http://faculty.ucmerced.edu/mhyang/">Ming-Hsuan Yang</a>, and
<a href="https://www.citi.sinica.edu.tw/pages/yylin/publications_en.html">Yen-Yu Lin</a> <br />
<i>Asian Conference on Computer Vision (ACCV), 2018</i> <br />
[<a href="http://faculty.ucmerced.edu/mhyang/papers/accv2018_matching.pdf">Paper (PDF)</a>]
[<a href="https://yunchunchen.github.io/WeakMatchNet/">Project page</a>]
[<a href="https://github.com/YunChunChen/WeakMatchNet">Code</a>]
[<a href="supp/ACCV_2018_Matching_Poster.pdf">Poster</a>]</p>
<p>
<a class="github-button" href="https://github.com/YunChunChen/WeakMatchNet" data-icon="octicon-star" data-size="large" data-show-count="true" aria-label="Star YunChunChen/WeakMatchNet on GitHub">Star</a>
<a class="github-button" href="https://github.com/YunChunChen/WeakMatchNet/fork" data-icon="octicon-repo-forked" data-size="large" data-show-count="true" aria-label="Fork YunChunChen/WeakMatchNet on GitHub">Fork</a>
</p>
</td></tr></table>
<table class="imgtable"><tr><td>
<img src="images/projects/ASSETS 2017 HOMER.jpg" alt="" width="400px" />&nbsp;</td>
<td align="left"><p><b>Semi-Automated Home-based Therapy for the Upper Extremity of Stroke Survivors</b> <br />
<a href="https://www.provost.vt.edu/">Thanassis Rikakis</a>,
<a href="http://people.cs.vt.edu/~aislingk/">Aisling Kelliher</a>,
<a href="https://sites.google.com/site/jchoivision/">Jinwoo Choi</a>,
<a href="http://www.jiabinhuang.com">Jia-Bin Huang</a>,
<a href="http://www.cs.cmu.edu/~kkitani/">Kris Kitani</a>,
<a href="http://www.rehabmed.emory.edu/faculty.bios/wolf-steven.html">Steve Wolf</a>, and
<a href="http://vtinr.com/portfolio/setor-zilevu/">Setor Zilevu</a> <br />
<i>Pervasive Technologies Related to Assistive Environments (PETRA), 2018</i><br />
[<a href="http://people.cs.vt.edu/~aislingk/papers/p249-Rikakis.pdf">Paper (PDF)</a>]</p>
</td></tr></table>
<table class="imgtable"><tr><td>
<img src="images/projects/Interactions 2018.jpg" alt="" width="400px" />&nbsp;</td>
<td align="left"><p><b>Progressive Cyber-Human Intelligence for Social Good</b> <br />
<a href="https://www.provost.vt.edu/">Thanassis Rikakis</a>,
<a href="http://people.cs.vt.edu/~aislingk/">Aisling Kelliher</a>,
<a href="http://www.jiabinhuang.com">Jia-Bin Huang</a>,
<a href="http://sundaram.cs.illinois.edu/">Hari Sundaram</a> <br />
<i>ACM Interactions, 2018</i><br />
[<a href="https://dl.acm.org/citation.cfm?id=3231559">Paper (PDF)</a>]</p>
</td></tr></table>
<table class="imgtable"><tr><td>
<img src="images/projects/CVPR 2017 SR.gif" alt="" width="400px" />&nbsp;</td>
<td align="left"><p><b>Fast and Accurate Image Super-Resolution with Deep Laplacian Pyramid Networks</b> <br />
<a href="http://graduatestudents.ucmerced.edu/wlai24/">Wei-Sheng Lai</a>,
<a href="http://www.jiabinhuang.com">Jia-Bin Huang</a>,
<a href="http://vision.ai.illinois.edu/ahuja.html">Narendra Ahuja</a>,
and <a href="http://faculty.ucmerced.edu/mhyang/">Ming-Hsuan Yang</a>  <br />
<i>IEEE Transactions on Pattern Analysis and Machine Intelligence (PAMI), 2018</i> <br />
[<a href="https://arxiv.org/abs/1710.01992">Paper (PDF)</a>]
[<a href="http://vllab1.ucmerced.edu/~wlai24/LapSRN/">Project page</a>]
[<a href="https://github.com/phoenix104104/LapSRN">Code</a>]</p>
<p>
<a class="github-button" href="https://github.com/phoenix104104/LapSRN" data-icon="octicon-star" data-size="large" data-show-count="true" aria-label="Star phoenix104104/LapSRN on GitHub">Star</a>
<a class="github-button" href="https://github.com/phoenix104104/LapSRN/fork" data-icon="octicon-repo-forked" data-size="large" data-show-count="true" aria-label="Fork phoenix104104/LapSRN on GitHub">Fork</a>
</p>
</td></tr></table>
<table class="imgtable"><tr><td>
<img src="images/projects/ICCV 2015 Tracking.gif" alt="" width="400px" />&nbsp;</td>
<td align="left"><p><b>Robust Visual Tracking via Hierarchical Convolutional Features</b> <br />
<a href="https://sites.google.com/site/chaoma99/">Chao Ma</a>,
<a href="http://www.jiabinhuang.com">Jia-Bin Huang</a>,
<a href="http://english.seiee.sjtu.edu.cn/english/detail/842_802.htm">Xiaokang Yang</a>, and
<a href="http://faculty.ucmerced.edu/mhyang/">Ming-Hsuan Yang</a>  <br />
<i>IEEE Transactions on Pattern Analysis and Machine Intelligence (PAMI), 2018</i> <br />
[<a href="https://arxiv.org/pdf/1707.03816.pdf">Paper (PDF)</a>]
[<a href="https://sites.google.com/site/chaoma99/hcft-tracking">Project page</a>]
[<a href="https://github.com/chaoma99/HCFTstar">Code</a>]</p>
<p>
<a class="github-button" href="https://github.com/chaoma99/HCFTstar" data-icon="octicon-star" data-size="large" data-show-count="true" aria-label="Star chaoma99/HCFTstar on GitHub">Star</a>
<a class="github-button" href="https://github.com/chaoma99/HCFTstar/fork" data-icon="octicon-repo-forked" data-size="large" data-show-count="true" aria-label="Fork chaoma99/HCFTstar on GitHub">Fork</a>
</p>
</td></tr></table>
<table class="imgtable"><tr><td>
<img src="images/projects/arXiv 2017 Long-term Tracking.jpg" alt="" width="400px" />&nbsp;</td>
<td align="left"><p><b>Adaptive Correlation Filters with Long-Term and Short-Term Memory for Object Tracking</b> <br />
<a href="https://sites.google.com/site/chaoma99/">Chao Ma</a>,
<a href="http://www.jiabinhuang.com">Jia-Bin Huang</a>,
<a href="https://scholar.google.com/citations?user=yDEavdMAAAAJ&amp;hl=en">Xiaokang Yang</a>, and
<a href="http://faculty.ucmerced.edu/mhyang/">Ming-Hsuan Yang</a>  <br />
<i>International Journal of Computer Vision (IJCV), 2018</i> <br />
[<a href="https://arxiv.org/abs/1707.02309">Paper (PDF)</a>]
[<a href="https://sites.google.com/site/chaoma99/cf-lstm">Project page</a>]
[<a href="https://github.com/chaoma99/lct-tracker">Code</a>]</p>
<p>
<a class="github-button" href="https://github.com/chaoma99/lct-tracker" data-icon="octicon-star" data-size="large" data-show-count="true" aria-label="Star chaoma99/lct-tracker on GitHub">Star</a>
<a class="github-button" href="https://github.com/chaoma99/lct-tracker/fork" data-icon="octicon-repo-forked" data-size="large" data-show-count="true" aria-label="Fork chaoma99/lct-tracker on GitHub">Fork</a>
</p>
</td></tr></table>
<table class="imgtable"><tr><td>
<img src="images/projects/CVIU 2018 Pose.jpg" alt="" width="400px" />&nbsp;</td>
<td align="left"><p><b>Ensemble of Convolutional Neural Networks for Pose Estimation</b> <br />
<a href="">Yuki Kawana</a>,
<a href="http://www.toyota-ti.ac.jp/Lab/Denshi/iim/ukita/">Norimichi Ukita</a>,
<a href="http://www.jiabinhuang.com">Jia-Bin Huang</a>, and
<a href="http://faculty.ucmerced.edu/mhyang/">Ming-Hsuan Yang</a>  <br />
<i>Computer Vision and Image Understanding (CVIU), 2018</i> <br />
[<a href="https://faculty.ucmerced.edu/mhyang/papers/cviu18_pose_estimation.pdf">Paper (PDF)</a>]</p>
</td></tr></table>
<table class="imgtable"><tr><td>
<img src="images/projects/NIPS 2017 Semi-supervised Flow.gif" alt="" width="400px" />&nbsp;</td>
<td align="left"><p><b>Semi-Supervised Learning for Optical Flow with Generative Adversarial Networks</b> <br />
<a href="http://graduatestudents.ucmerced.edu/wlai24/">Wei-Sheng Lai</a>,
<a href="http://www.jiabinhuang.com">Jia-Bin Huang</a>, and
<a href="http://faculty.ucmerced.edu/mhyang/">Ming-Hsuan Yang</a>  <br />
<i>Proceedings of Neural Information Processing Systems (NeurIPS), 2017</i> <br />
[<a href="https://papers.nips.cc/paper/6639-semi-supervised-learning-for-optical-flow-with-generative-adversarial-networks.pdf">Paper (PDF)</a>]
[<a href="http://vllab.ucmerced.edu/wlai24/semiFlowGAN/">Project page</a>]
[<a href="">Code</a>]
[<a href="supp/NeurIPS_2017_SemiFlow_Poster.pptx">Poster</a>]
[<a href="supp/NeurIPS_2017_SemiFlow_Supp.pdf">Supp</a>]</p>
</td></tr></table>
<table class="imgtable"><tr><td>
<img src="images/projects/NIPS 2017 Video Segmentation.gif" alt="" width="400px" />&nbsp;</td>
<td align="left"><p><b>MaskRNN: Instance Level Video Object Segmentation</b> <br />
<a href="https://sites.google.com/view/yuantinghu">Yuan-Ting Hu</a>,
<a href="http://www.jiabinhuang.com">Jia-Bin Huang</a>, and
<a href="http://www.alexander-schwing.de/">Alexander Schwing</a>  <br />
<i>Proceedings of Neural Information Processing Systems (NeurIPS), 2017</i> <br />
[<a href="http://papers.nips.cc/paper/6636-maskrnn-instance-level-video-object-segmentation.pdf">Paper (PDF)</a>]
[<a href="https://sites.google.com/view/yuantinghu/maskrnn">Project page</a>]
[<a href="">Code</a>]
[<a href="supp/NeurIPS_2017_VideoSeg_Poster.pptx">Poster</a>]</p>
</td></tr></table>
<table class="imgtable"><tr><td>
<img src="images/projects/arXiv 2017 Adversarial Examples.gif" alt="" width="400px" />&nbsp;</td>
<td align="left"><p><b>Detecting Adversarial Attacks on Neural Network Policies with Visual Foresight</b> <br />
<a href="https://yenchenlin.me/">Yen-Chen Lin</a>,
<a href="http://mingyuliu.net/">Ming-Yu Liu</a>,
<a href="http://aliensunmin.github.io/">Min Sun</a>, and
<a href="http://www.jiabinhuang.com">Jia-Bin Huang</a><br />
<i>Proceedings of Neural Information Processing Systems (NeurIPS) Workshop on Machine Deception, 2017</i> <br />
[<a href="https://arxiv.org/abs/1710.00814">Paper (PDF)</a>]
[<a href="https://yenchenlin.me/RL_attack_detection/">Project page</a>]
[<a href="https://github.com/yenchenlin/rl-attack-detection">Code</a>]</p>
<p>
<a class="github-button" href="https://github.com/yenchenlin/rl-attack-detection " data-icon="octicon-star" data-size="large" data-show-count="true" aria-label="Star yenchenlin/rl-attack-detection on GitHub">Star</a>
<a class="github-button" href="https://github.com/yenchenlin/rl-attack-detection/fork" data-icon="octicon-repo-forked" data-size="large" data-show-count="true" aria-label="Fork yenchenlin/rl-attack-detection on GitHub">Fork</a>
</p>
</td></tr></table>
<table class="imgtable"><tr><td>
<img src="images/projects/ICCV 2017 Sorting.gif" alt="" width="400px" />&nbsp;</td>
<td align="left"><p><b>Unsupervised Representation Learning by Sorting Sequences</b> <br />
<a href="http://vllab1.ucmerced.edu/~hylee/">Hsin-Ying Lee</a>,
<a href="http://www.jiabinhuang.com">Jia-Bin Huang</a>,
<a href="https://www.researchgate.net/profile/Maneesh_Singh5">Maneesh Singh</a>, and
<a href="http://faculty.ucmerced.edu/mhyang/">Ming-Hsuan Yang</a>  <br />
<i>Proceedings of International Conference on Computer Vision (ICCV), 2017</i> <br />
[<a href="http://vllab.ucmerced.edu/hylee/publication/ICCV17_OPN.pdf">Paper (PDF)</a>]
[<a href="http://vllab.ucmerced.edu/hylee/OPN/">Project page</a>]
[<a href="http://github.com/HsinYingLee/OPN">Code</a>]
[<a href="supp/ICCV_2017_OPN_Poster.pptx">Poster</a>]</p>
<p>
<a class="github-button" href="https://github.com/HsinYingLee/OPN " data-icon="octicon-star" data-size="large" data-show-count="true" aria-label="Star HsinYingLee/OPN on GitHub">Star</a>
<a class="github-button" href="https://github.com/HsinYingLee/OPN/fork" data-icon="octicon-repo-forked" data-size="large" data-show-count="true" aria-label="Fork HsinYingLee/OPN on GitHub">Fork</a>
</p>
</td></tr></table>
<table class="imgtable"><tr><td>
<img src="images/projects/CVPR 2017 SR.gif" alt="" width="400px" />&nbsp;</td>
<td align="left"><p><b>Deep Laplacian Pyramid Networks for Fast and Accurate Super-Resolution</b> <br />
<a href="http://graduatestudents.ucmerced.edu/wlai24/">Wei-Sheng Lai</a>,
<a href="http://www.jiabinhuang.com">Jia-Bin Huang</a>,
<a href="http://vision.ai.illinois.edu/ahuja.html">Narendra Ahuja</a>,
and <a href="http://faculty.ucmerced.edu/mhyang/">Ming-Hsuan Yang</a>  <br />
<i>Proceedings of IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2017</i> <br />
[<a href="http://vllab.ucmerced.edu/wlai24/LapSRN/papers/cvpr17_LapSRN.pdf">Paper (PDF)</a>]
[<a href="http://vllab.ucmerced.edu/wlai24/LapSRN/">Project page</a>]
[<a href="https://github.com/phoenix104104/LapSRN">Code</a>]
[<a href="supp/CVPR_2017_LapSRN_Poster.pptx">Poster</a>]
[<a href="supp/CVPR_2017_LapSRN_Supp.pdf">Supp</a>]</p>
<p>
<a class="github-button" href="https://github.com/phoenix104104/LapSRN " data-icon="octicon-star" data-size="large" data-show-count="true" aria-label="Star phoenix104104/LapSRN on GitHub">Star</a>
<a class="github-button" href="https://github.com/phoenix104104/LapSRN/fork" data-icon="octicon-repo-forked" data-size="large" data-show-count="true" aria-label="Fork phoenix104104/LapSRN on GitHub">Fork</a>
</p>
</td></tr></table>
<table class="imgtable"><tr><td>
<img src="images/projects/ASSETS 2017 HOMER.jpg" alt="" width="400px" />&nbsp;</td>
<td align="left"><p><b>HOMER: an interactive system for home based stroke rehabilitation</b> <br />
<a href="http://people.cs.vt.edu/~aislingk/">Aisling Kelliher</a>,
<a href="https://sites.google.com/site/jchoivision/">Jinwoo Choi</a>,
<a href="http://www.jiabinhuang.com">Jia-Bin Huang</a>,
<a href="http://www.cs.cmu.edu/~kkitani/">Kris Kitani</a>, and
<a href="https://www.provost.vt.edu/">Thanassis Rikakis</a> <br />
<i>Proceedings of International ACM SIGACCESS Conference on Computers and Accessibility (ASSETS), 2017</i> <br />
[<a href="http://delivery.acm.org/10.1145/3140000/3134807/p379-kelliher.pdf">Paper (PDF)</a>]
[<a href="http://vtinr.com/people/">Project page</a>]</p>
</td></tr></table>
<table class="imgtable"><tr><td>
<img src="images/misc/thesis_evolution.gif" alt="" width="400px" />&nbsp;</td>
<td align="left"><p><b>Visual Analysis and Synthesis with Physically Grounded Constraints</b> <br />
<a href="http://www.jiabinhuang.com">Jia-Bin Huang</a> <br />
Ph.D. Thesis, Department of Electrical and Computer Engineering <br />
University of Illinois, Urbana-Champaign, August 2016 <br />
[<a href="papers/PhDThesis_2016_Jia-Bin.pdf">Paper (PDF)</a>]
[<a href="poster/CVPR_2016_Doctoral_Consortium_Poster.pdf">Poster</a>]</p>
</td></tr></table>
<table class="imgtable"><tr><td>
<img src="images/projects/SigAsia 2016 Video Completion.gif" alt="" width="400px" />&nbsp;</td>
<td align="left"><p><b>Temporally Coherent Completion of Dynamic Video</b> <br />
<a href="http://www.jiabinhuang.com">Jia-Bin Huang</a>,
<a href="http://www.singbingkang.com">Sing Bing Kang</a>,
<a href="http://vision.ai.illinois.edu/ahuja.html">Narendra Ahuja</a>, and
<a href="http://johanneskopf.de/">Johannes Kopf</a> <br />
<i>ACM Transactions on Graphics (Proceedings of SIGGRAPH Asia), 2016</i> <br />
[<a href="papers/SigAsia_2016_VideoCompletion.pdf">Paper (PDF)</a>]
[<a href="https://filebox.ece.vt.edu/~jbhuang/project/vidcomp/">Project page</a>]
[<a href="https://filebox.ece.vt.edu/~jbhuang/project/vidcomp/video_completion_source.zip">Code</a>]
[<a href="https://filebox.ece.vt.edu/~jbhuang/project/vidcomp/video_completion_mask.zip">Mask</a>]
[<a href="https://filebox.ece.vt.edu/~jbhuang/project/vidcomp/video_completion_results.zip">Results</a>]</p>
</td></tr></table>
<table class="imgtable"><tr><td>
<img src="images/projects/ECCV 2016 Deep Joint Image Filter.gif" alt="" width="400px" />&nbsp;</td>
<td align="left"><p><b>Deep Joint Image Filter</b> <br />
<a href="https://sites.google.com/site/yijunlimaverick/">Yijun Li</a>,
<a href="http://www.jiabinhuang.com">Jia-Bin Huang</a>,
<a href="http://vision.ai.illinois.edu/ahuja.html">Narendra Ahuja</a>, and
<a href="http://faculty.ucmerced.edu/mhyang/">Ming-Hsuan Yang</a> <br />
<i>Proceedings of European Conference on Computer Vision (ECCV), 2016</i> <br />
[<a href="https://uofi.box.com/shared/static/t248om34hbqedo735nkz3zkztgd23wu6.pdf">Paper (PDF)</a>]
[<a href="https://sites.google.com/site/yijunlimaverick/deepjointfilter">Project page</a>]
[<a href="https://github.com/Yijunmaverick/DeepJointFilter">Code</a>]
[<a href="https://uofi.box.com/shared/static/0dbi3saznben0sspkm7k7h8ka73hyzrr.pdf">Supp</a>]
[<a href="poster/ECCV_2016_JointFilter_Poster.pdf">Poster</a>]</p>
<p>
<a class="github-button" href="https://github.com/Yijunmaverick/DeepJointFilter " data-icon="octicon-star" data-size="large" data-show-count="true" aria-label="Star Yijunmaverick/DeepJointFilter on GitHub">Star</a>
<a class="github-button" href="https://github.com/Yijunmaverick/DeepJointFilter/fork" data-icon="octicon-repo-forked" data-size="large" data-show-count="true" aria-label="Fork Yijunmaverick/DeepJointFilter on GitHub">Fork</a>
</p>
</td></tr></table>
<table class="imgtable"><tr><td>
<img src="images/projects/ECCV 2016 Face Tracking.jpg" alt="" width="400px" />&nbsp;</td>
<td align="left"><p><b>Tracking Persons-of-Interest via Adaptive Discriminative Features</b> <br />
<a href="http://shunzhang.me.pn/">Shun  Zhang</a>,
<a href="https://scholar.google.com/citations?user=x2xdU7gAAAAJ&amp;hl=en">Yihong Gong</a>,
<a href="http://www.jiabinhuang.com">Jia-Bin Huang</a>,
<a href="http://cvlab.hanyang.ac.kr/~jwlim/">Jongwoo Lim</a>,
<a href="http://xypeng.gr.xjtu.edu.cn/web/jinjun/english">Jinjun Wang</a>,
<a href="http://vision.ai.illinois.edu/ahuja.html">Narendra Ahuja</a>, and
<a href="http://faculty.ucmerced.edu/mhyang/">Ming-Hsuan Yang</a> <br />
<i>Proceedings of European Conference on Computer Vision (ECCV), 2016</i> <br />
[<a href="https://uofi.box.com/shared/static/km2k1wrc8f9w2eysns3643d7sp5z1ge9.pdf">Paper (PDF)</a>]
[<a href="https://sites.google.com/site/shunzhang876/eccv16_facetracking">Project page</a>]
[<a href="https://github.com/shunzhang876/AdaptiveFeatureLearning">Code</a>]
[<a href="https://uofi.box.com/shared/static/p7yrhjsxjfasbvpfxwi7waapgkd6hi6j.pdf">Supp</a>]
[<a href="poster/ECCV_2016_FaceTracking_Poster.pdf">Poster</a>]</p>
<p>
<a class="github-button" href="https://github.com/shunzhang876/AdaptiveFeatureLearning " data-icon="octicon-star" data-size="large" data-show-count="true" aria-label="Star shunzhang876/AdaptiveFeatureLearning on GitHub">Star</a>
<a class="github-button" href="https://github.com/shunzhang876/AdaptiveFeatureLearning/fork" data-icon="octicon-repo-forked" data-size="large" data-show-count="true" aria-label="Fork shunzhang876/AdaptiveFeatureLearning on GitHub">Fork</a>
</p>
</td></tr></table>
<table class="imgtable"><tr><td>
<img src="images/projects/ECCV 2016 Feature Learning.jpg" alt="" width="400px" />&nbsp;</td>
<td align="left"><p><b>Unsupervised Visual Representation Learning by Graph-based Consistent Constraints</b> <br />
<a href="https://sites.google.com/site/lidonggg930">Dong Li</a>,
<a href="">Wei-Chih Hung</a>,
<a href="http://www.jiabinhuang.com">Jia-Bin Huang</a>,
<a href="http://www.tsinghua.edu.cn/publish/eeen/3784/2010/20101219115601212198627/20101219115601212198627_.html">Shengjin Wang</a>,
<a href="http://vision.ai.illinois.edu/ahuja.html">Narendra Ahuja</a>,
and <a href="http://faculty.ucmerced.edu/mhyang/">Ming-Hsuan Yang</a>  <br />
<i>Proceedings of European Conference on Computer Vision (ECCV), 2016</i> <br />
[<a href="https://uofi.box.com/shared/static/onxj2h1lld86155gjeamb8n5oxg6krrh.pdf">Paper (PDF)</a>]
[<a href="https://sites.google.com/site/lidonggg930/feature-learning">Project page</a>]
[<a href="https://github.com/dongli12/FeatureLearning">Code</a>]
[<a href="https://uofi.box.com/shared/static/bw2ife71tu0e19l04m716w14j4n1nlmb.pdf">Supp</a>]
[<a href="slides/ECCV_2016_RepresentationLearning_Slides.pptx">Slides</a>]
[<a href="poster/ECCV_2016_RepresentationLearning_Poster.pdf">Poster</a>]</p>
<p>
<a class="github-button" href="https://github.com/dongli12/FeatureLearning" data-icon="octicon-star" data-size="large" data-show-count="true" aria-label="Star dongli12/FeatureLearning on GitHub">Star</a>
<a class="github-button" href="https://github.com/dongli12/FeatureLearning/fork" data-icon="octicon-repo-forked" data-size="large" data-show-count="true" aria-label="Fork dongli12/FeatureLearning on GitHub">Fork</a>
</p>
</td></tr></table>
<table class="imgtable"><tr><td>
<img src="images/projects/CVPR 2016 Bird.jpg" alt="" width="400px" />&nbsp;</td>
<td align="left"><p><b>Detecting Migrating Birds at Night</b> <br />
<a href="http://www.jiabinhuang.com">Jia-Bin Huang</a>, <a href="http://www.cs.cornell.edu/~caruana/">Rich Caruana</a>, <a href="http://birdcast.info/person/farnsworth/">Andrew Farnsworth</a>, <a href="http://www.birds.cornell.edu/Page.aspx?pid=1735&amp;id=126">Steve Kelling</a>, and <a href="http://vision.ai.illinois.edu/ahuja.html">Narendra Ahuja</a> <br />
<i>Proceedings of IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2016</i> <br />
[<a href="https://uofi.box.com/shared/static/k7ysd31dn0q2bakazpwqqtep91tonw3z.pdf">Paper (PDF)</a>]
[<a href="https://sites.google.com/site/jbhuang0604/publications/bird_detection">Project page</a>]
[<a href="poster/CVPR_2016_BirdDetection_Poster.pdf">Poster</a>]</p>
</td></tr></table>
<table class="imgtable"><tr><td>
<img src="images/projects/CVPR 2016 Deblur.jpg" alt="" width="400px" />&nbsp;</td>
<td align="left"><p><b>A Comparative Study for Single Image Blind Deblurring</b> <br />
<a href="http://graduatestudents.ucmerced.edu/wlai24/">Wei-Sheng Lai</a>,
<a href="http://www.jiabinhuang.com">Jia-Bin Huang</a>,
<a href="https://eng.ucmerced.edu/people/zhu">Zhe Hu</a>,
<a href="http://vision.ai.illinois.edu/ahuja.html">Narendra Ahuja</a>,
and <a href="http://faculty.ucmerced.edu/mhyang/">Ming-Hsuan Yang</a>  <br />
<i>Proceedings of IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2016</i> (Spotlight Oral)<br />
[<a href="https://uofi.box.com/shared/static/nabooe7bv7ccc5oo5jvl7dg59cj7j0he.pdf">Paper (PDF)</a>]
[<a href="http://vllab1.ucmerced.edu/~wlai24/cvpr16_deblur_study/">Project page</a>]
[<a href="https://github.com/phoenix104104/cvpr16_deblur_study">Code</a>]
[<a href="https://uofi.box.com/shared/static/ivtgah4g2porwcdvgcx11682j7ti2p86.pdf">Supp</a>]
[<a href="slides/CVPR_2016_DeblurStudy_Slides.pdf">Slides</a>]
[<a href="poster/CVPR_2016_DeblurStudy_Poster.pdf">Poster</a>]</p>
<p>
<a class="github-button" href="https://github.com/phoenix104104/cvpr16_deblur_study" data-icon="octicon-star" data-size="large" data-show-count="true" aria-label="Star phoenix104104/cvpr16_deblur_study on GitHub">Star</a>
<a class="github-button" href="https://github.com/phoenix104104/cvpr16_deblur_study/fork" data-icon="octicon-repo-forked" data-size="large" data-show-count="true" aria-label="Fork phoenix104104/cvpr16_deblur_study on GitHub">Fork</a>
</p>
</td></tr></table>
<table class="imgtable"><tr><td>
<img src="images/projects/CVPR 2016 WSL.jpg" alt="" width="400px" />&nbsp;</td>
<td align="left"><p><b>Weakly Supervised Object Localization with Progressive Domain Adaptation</b> <br />
<a href="https://sites.google.com/site/lidonggg930">Dong Li</a>,
<a href="http://www.jiabinhuang.com">Jia-Bin Huang</a>,
<a href="http://dblp.uni-trier.de/pers/hd/l/Li:Yali">Yali Li</a>,
<a href="http://www.tsinghua.edu.cn/publish/eeen/3784/2010/20101219115601212198627/20101219115601212198627_.html">Shengjin Wang</a>,
and <a href="http://faculty.ucmerced.edu/mhyang/">Ming-Hsuan Yang</a>  <br />
<i>Proceedings of IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2016</i> <br />
[<a href="https://uofi.box.com/shared/static/3zuynl7mc72zqr65z4xubeihe43chjom.pdf">Paper (PDF)</a>]
[<a href="https://sites.google.com/site/lidonggg930/wsl">Project page</a>]
[<a href="https://github.com/jbhuang0604/WSL/">Code</a>]
[<a href="https://uofi.box.com/shared/static/rpnk0b5nxe7uhltotomnyf2vh162ldtf.pdf">Supp</a>]
[<a href="poster/CVPR_2016_WSL_poster.pdf">Poster</a>]</p>
<p>
<a class="github-button" href="https://github.com/jbhuang0604/WSL/" data-icon="octicon-star" data-size="large" data-show-count="true" aria-label="Star jbhuang0604/WSL on GitHub">Star</a>
<a class="github-button" href="https://github.com/jbhuang0604/WSL/fork" data-icon="octicon-repo-forked" data-size="large" data-show-count="true" aria-label="Fork jbhuang0604/WSL on GitHub">Fork</a>
</p>
</td></tr></table>
<table class="imgtable"><tr><td>
<img src="images/projects/ICCV 2015 Tracking.gif" alt="" width="400px" />&nbsp;</td>
<td align="left"><p><b>Hierarchical Convolutional Features for Visual Tracking</b> <br />
<a href="https://sites.google.com/site/chaoma99/">Chao Ma</a>, <a href="http://www.jiabinhuang.com">Jia-Bin Huang</a>, <a href="http://english.seiee.sjtu.edu.cn/english/detail/842_802.htm">Xiaokang Yang</a>, and <a href="http://faculty.ucmerced.edu/mhyang/">Ming-Hsuan Yang</a>  <br />
<i>Proceedings of IEEE International Conference on Computer Vision (ICCV), 2015</i> <br />
[<a href="https://uofi.box.com/shared/static/o8wkllte8sfyuvt8ei77o24we8l36qoj.pdf">Paper (PDF)</a>]
[<a href="https://sites.google.com/site/jbhuang0604/publications/cf2">Project page</a>]
[<a href="https://github.com/jbhuang0604/CF2">Code</a>]
[<a href="https://uofi.box.com/shared/static/6y3izswn40y6ckbwgm40ugledzp8fer9.pdf">Supp</a>]
[<a href="slides/ICCV_2015_HCF_Slides.pdf">Slides</a>]
[<a href="poster/ICCV_2015_HCF_Poster.pdf">Poster</a>]</p>
<p>
<a class="github-button" href="https://github.com/jbhuang0604/CF2" data-icon="octicon-star" data-size="large" data-show-count="true" aria-label="Star jbhuang0604/CF2 on GitHub">Star</a>
<a class="github-button" href="https://github.com/jbhuang0604/CF2/fork" data-icon="octicon-repo-forked" data-size="large" data-show-count="true" aria-label="Fork jbhuang0604/CF2 on GitHub">Fork</a>
</p>
</td></tr></table>
<table class="imgtable"><tr><td>
<img src="images/projects/CVPR 2015 SelfExSR.gif" alt="" width="400px" />&nbsp;</td>
<td align="left"><p><b>Single Image Super-Resolution from Transformed Self-Exemplars</b> <br />
<a href="http://www.jiabinhuang.com">Jia-Bin Huang</a>,
<a href="https://sites.google.com/site/abhishek486/">Abhishek Singh</a>, and
<a href="http://vision.ai.illinois.edu/ahuja.html">Narendra Ahuja</a> <br />
<i>Proceedings of IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2015</i> (Oral Presentation) <br />
[<a href="https://uofi.box.com/shared/static/8llt4ijgc39n3t7ftllx7fpaaqi3yau0.pdf">Paper (PDF)</a>]
[<a href="https://sites.google.com/site/jbhuang0604/publications/struct_sr">Project page</a>]
[<a href="https://github.com/jbhuang0604/SelfExSR">Code</a>]
[<a href="https://uofi.box.com/shared/static/k0a2wziaavwoo557aygkbwsa6syln1b9.pdf">Supp</a>]
[<a href="http://techtalks.tv/talks/single-image-super-resolution-from-transformed-self-exemplars/61635/">Talk</a>]
[<a href="poster/CVPR_2015_SelfExSR_Slides.pptx">Slides</a>]
[<a href="poster/CVPR_2015_SelfExSR_Poster.pdf">Poster</a>]</p>
<p>
<a class="github-button" href="https://github.com/jbhuang0604/SelfExSR" data-icon="octicon-star" data-size="large" data-show-count="true" aria-label="Star jbhuang0604/SelfExSR on GitHub">Star</a>
<a class="github-button" href="https://github.com/jbhuang0604/SelfExSR/fork" data-icon="octicon-repo-forked" data-size="large" data-show-count="true" aria-label="Fork jbhuang0604/SelfExSR on GitHub">Fork</a>
</p>
</td></tr></table>
<table class="imgtable"><tr><td>
<img src="images/projects/SIGGRAPH 2014 Completion.jpg" alt="" width="400px" />&nbsp;</td>
<td align="left"><p><b>Image Completion using Planar Structure Guidance</b> <br />
<a href="http://www.jiabinhuang.com">Jia-Bin Huang</a>,
<a href="http://www.singbingkang.com">Sing Bing Kang</a>,
<a href="http://vision.ai.illinois.edu/ahuja.html">Narendra Ahuja</a>, and
<a href="http://johanneskopf.de/">Johannes Kopf</a> <br />
<i>ACM Transactions on Graphics, Volume 33, Number 4 (SIGGRAPH), 2014</i> (Oral Presentation) <br />
[<a href="https://uofi.box.com/shared/static/hrtyccvriwq9fjc0ryzb.pdf">Paper (PDF)</a>]
[<a href="https://sites.google.com/site/jbhuang0604/publications/struct_completion">Project page</a>]
[<a href="supp/SIGGRAPH">2014 Completion_Slides.pdf Slides</a>]
[<a href="supp/SIGGRAPH_2014_Completion_Supp.zip">Supp</a>]</p>
</td></tr></table>
<table class="imgtable"><tr><td>
<img src="images/projects/ETRA 2014 EyeTracking.jpg" alt="" width="400px" />&nbsp;</td>
<td align="left"><p><b>Towards Accurate and Robust Cross-Ratio based Gaze Trackers Through Learning From Simulation</b> <br />
<a href="http://www.jiabinhuang.com">Jia-Bin Huang</a>, <a href="http://research.microsoft.com/en-us/people/qincai/">Qin Cai</a>, <a href="http://research.microsoft.com/en-us/um/people/zliu/">Zicheng Liu</a>, <a href="http://vision.ai.illinois.edu/ahuja.html">Narendra Ahuja</a>, and <a href="http://research.microsoft.com/en-us/um/people/zhang/">Zhengyou Zhang</a> <br />
<i>Proceedings of ACM Symposium on Eye Tracking Research &amp; Applications (ETRA), 2014</i> (Oral Presentation) <br />
[<a href="https://uofi.box.com/shared/static/j7wucb2yqjfe30qjc2ic.pdf">Paper (PDF)</a>]
[<a href="https://sites.google.com/site/jbhuang0604/publications/eyetracking_etra_2014">Project page</a>]
[<a href="supp/ETRA_2014_EyeTracking_Slides.pptx">Slides</a>]
<br />
<b><a href="https://dl.dropboxusercontent.com/u/2810224/Homepage/publications/2014/Eye_Tracking_ETRA_2014/ETRA_Award.jpg">ETRA 2014 Best Paper Award</a></b>
[Patent]: <a href="http://www.freepatentsonline.com/y2015/0278599.html">Eye Gaze Tracking based upon Adaptive Homography Mapping</a></p>
</td></tr></table>
<table class="imgtable"><tr><td>
<img src="images/projects/ICCP 2013 Completion.jpg" alt="" width="400px" />&nbsp;</td>
<td align="left"><p><b>Transformation Guided Image Completion</b> <br />
<a href="http://www.jiabinhuang.com">Jia-Bin Huang</a>, <a href="http://johanneskopf.de/">Johannes Kopf</a>, <a href="http://vision.ai.illinois.edu/ahuja.html">Narendra Ahuja</a>, and <a href="http://www.singbingkang.com">Sing Bing Kang</a> <br />
<i>Proceedings of IEEE Conference on International Conference on Computational Photography (ICCP), 2013</i> (Oral Presentation) <br />
[<a href="https://uofi.box.com/shared/static/vg33ylal17ol7ptt3xrx.pdf">Paper (PDF)</a>]
[<a href="https://sites.google.com/site/jbhuang0604/publications/completion_iccp_2013">Project page</a>]
[<a href="supp/ICCP_2013_Completion_Slides.pdf">Slides</a>]
[<a href="supp/ICCP_2013_Completion_Supp.pdf">Supplementary material</a>] <br /></p>
</td></tr></table>
<table class="imgtable"><tr><td>
<img src="https://sites.google.com/site/jbhuang0604/_/rsrc/1354775999564/publications/saliency.png?height=246&amp;width=400" alt="" width="400px" />&nbsp;</td>
<td align="left"><p><b>Saliency Detection via Divergence Analysis: A Unified Perspective</b> <br />
<a href="http://www.jiabinhuang.com">Jia-Bin Huang</a> and <a href="http://vision.ai.illinois.edu/ahuja.html">Narendra Ahuja</a> <br />
<i>Proceedings of International Conference on Pattern Recognition (ICPR), 2012</i> (Oral Presentation) <br />
[<a href="https://uofi.box.com/shared/static/9o2jqbnxyk56h0aqz5010p2v0jfoc5mi.pdf">Paper (PDF)</a>]
[<a href="https://sites.google.com/site/jbhuang0604/publications/saliency_icpr_2012">Project page</a>]
[<a href="supp/ICPR_2012_Saliency_Slides.pdf">Slides</a>]
<br /></p>
<p><b><a href="http://icpr2012.org/prizes-awards.html#beststudent">ICPR 2012 Best Student Paper Award (Computer and Robotic Vision Track)</a></b></p>
</td></tr></table>
<table class="imgtable"><tr><td>
<img src="images/projects/ACCV 2010 SR.jpg" alt="" width="400px" />&nbsp;</td>
<td align="left"><p><b>Exploiting Self-Similarities for Single Frame Super-Resolution</b> <br />
<a href="https://eng.ucmerced.edu/people/cyang35">Chih-Yuan Yang</a>, <a href="http://www.jiabinhuang.com">Jia-Bin Huang</a>, and <a href="http://faculty.ucmerced.edu/mhyang/index.html">Ming-Hsuan Yang</a> <br />
<i>Proceedings of Asian Conference on Computer Vision (ACCV), 2010</i> <br />
[<a href="https://uofi.box.com/shared/static/7yvzotxsu6vnrq4ulo92pyzkip0ir5xd.pdf">Paper (PDF)</a>]
[<a href="https://sites.google.com/site/jbhuang0604/publications/superresolution_accv_2010">Project page</a>]
[<a href="supp/ACCV_2010_SuperRes_Poster.pdf">Poster</a>]
<br /></p>
</td></tr></table>
<table class="imgtable"><tr><td>
<img src="images/projects/ICIP 2010 Deblur.jpg" alt="" width="400px" />&nbsp;</td>
<td align="left"><p><b>Single Image Deblurring with Adaptive Dictionary Learning</b> <br />
<a href="https://eng.ucmerced.edu/people/zhu">Zhe Hu</a>, <a href="http://www.jiabinhuang.com">Jia-Bin Huang</a>, and <a href="http://faculty.ucmerced.edu/mhyang/">Ming-Hsuan Yang</a> <br />
<i>Proceedings of IEEE International Conference on Image Processing (ICIP), 2010.</i> <br />
[<a href="https://uofi.box.com/shared/static/htbgyhcars61araughz5a1ewmdfes9x9.pdf">Paper (PDF)</a>]
[<a href="https://eng.ucmerced.edu/people/zhu/AdaptDict.html">Project page</a>]
[<a href="supp/ICIP_2010_Deblur_Poster.pdf">Poster</a>]
<br /></p>
</td></tr></table>
<table class="imgtable"><tr><td>
<img src="images/projects/CVPR 2010 FSR.jpg" alt="" width="400px" />&nbsp;</td>
<td align="left"><p><b>Fast Sparse Representation with Prototypes</b> <br />
<a href="http://www.jiabinhuang.com">Jia-Bin Huang</a> and <a href="http://faculty.ucmerced.edu/mhyang/">Ming-Hsuan Yang</a> <br />
<i>Proceedings of IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2010.</i> <br />
[<a href="https://uofi.box.com/shared/static/w5emg3w4t04j4l25nm4ct1gk1vm8xluh.pdf">Paper (PDF)</a>]
[<a href="https://sites.google.com/site/jbhuang0604/publications/fsr_cvpr_2010">Project page</a>]
[<a href="supp/CVPR_2010_FSR_Poster.pdf">Poster</a>]
<br /></p>
</td></tr></table>
<table class="imgtable"><tr><td>
<img src="images/projects/ACCV 2009 Pose.jpg" alt="" width="400px" />&nbsp;</td>
<td align="left"><p><b>Estimating Human Pose from Occluded Images</b> <br />
<a href="http://www.jiabinhuang.com">Jia-Bin Huang</a> and <a href="http://faculty.ucmerced.edu/mhyang/">Ming-Hsuan Yang</a> <br />
<i>Proceedings of Asian Conference on Computer Vision (ACCV), 2009</i> (Oral Presentation) <br />
[<a href="https://uofi.box.com/shared/static/67wnk4n076vkzfnqo5poihkpypqf1fkb.pdf">Paper (PDF)</a> ]
[<a href="https://sites.google.com/site/jbhuang0604/publications/pose_accv_2009">Project page</a>]
[<a href="supp/ACCV_2009_Pose_Slides.pdf">Slides</a>]
<br /></p>
</td></tr></table>
<table class="imgtable"><tr><td>
<img src="images/projects/CVPR 2009 CastShadows.jpg" alt="" width="400px" />&nbsp;</td>
<td align="left"><p><b>Moving Cast Shadow Detection Using Physics-based Features</b> <br />
<a href="http://www.jiabinhuang.com">Jia-Bin Huang</a> and <a href="http://www.iis.sinica.edu.tw/pages/song/eindex.html">Chu-Song Chen</a> <br />
<i>Proceedings of IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2009</i> <br />
[<a href="https://uofi.box.com/shared/static/laa4vkoxk7em5cygxm448k7ty54kpwdi.pdf">Paper (PDF)</a>]
[<a href="https://sites.google.com/site/jbhuang0604/publications/shadow_cvpr_2009">Project page</a>]
[<a href="supp/CVPR_2009_CastShadows_Poster.pdf">Poster</a>]
<br /></p>
</td></tr></table>
<table class="imgtable"><tr><td>
<img src="images/projects/ICASSP 2009 CastShadows.jpg" alt="" width="400px" />&nbsp;</td>
<td align="left"><p><b>A Physical Approach to Moving Cast Shadow Detection</b> <br />
<a href="http://www.jiabinhuang.com">Jia-Bin Huang</a> and <a href="http://www.iis.sinica.edu.tw/pages/song/eindex.html">Chu-Song Chen</a> <br />
<i>Proceedings of IEEE International Conference on Acoustics, Speech and Signal Processing, (ICASSP), 2009</i> <br />
[<a href="https://uofi.box.com/shared/static/k82bp4qmz589yakj8kovzrviona6b2ab.pdf">Paper (PDF)</a>]
[<a href="https://sites.google.com/site/jbhuang0604/publications/castshadow_icassp_2009">Project page</a>] <br /></p>
</td></tr></table>
<table class="imgtable"><tr><td>
<img src="images/projects/ICASSP 2009 Recoloring.jpg" alt="" width="400px" />&nbsp;</td>
<td align="left"><p><b>Image Recolorization For The Colorblind</b> <br />
<a href="http://www.jiabinhuang.com">Jia-Bin Huang</a>, Chu-Song Chen Tzu-Cheng Jen, and <a href="http://vlab.ee.nctu.edu.tw/advisor/">Sheng-Jyh Wang</a> <br />
<i>Proceedings of IEEE International Conference on Acoustics, Speech and Signal Processing, (ICASSP), 2009</i> <br />
[<a href="https://uofi.box.com/shared/static/fhxd79owfyvkpeuigohw800wp7j9m5gt.pdf">Paper (PDF)</a>]
[<a href="https://sites.google.com/site/jbhuang0604/publications/cvd_icassp_2009">Project page</a>] <br /></p>
</td></tr></table>
<table class="imgtable"><tr><td>
<img src="images/projects/VS 2008 CastShadows.jpg" alt="" width="400px" />&nbsp;</td>
<td align="left"><p><b>Learning Moving Cast Shadows for Foreground Detection</b> <br />
<a href="http://www.jiabinhuang.com">Jia-Bin Huang</a> and <a href="http://www.iis.sinica.edu.tw/pages/song/eindex.html">Chu-Song Chen</a> <br />
<i>Eighth International Workshop on Visual Surveillance (VS), ECCV Workshop 2008</i> (Oral Presentation) <br />
[<a href="https://uofi.box.com/shared/static/5nd6dmnt15c67ky2nqk9vcn9g0yq54eo.pdf">Paper (PDF)</a>]
[<a href="https://sites.google.com/site/jbhuang0604/publications/castshadow_vs_2008">Project page</a>] <br /></p>
</td></tr></table>
<table class="imgtable"><tr><td>
<img src="images/projects/CVAVI 2008 Recoloring.jpg" alt="" width="400px" />&nbsp;</td>
<td align="left"><p><b>Enhancing Color Representation for the Color Vision Impaired</b> <br />
<a href="http://www.jiabinhuang.com">Jia-Bin Huang</a>, Sih-Ying Wu and <a href="http://www.iis.sinica.edu.tw/pages/song/eindex.html">Chu-Song Chen</a> <br />
<i>Workshop on Computer Vision Applications for the Visually Impaired (CVAVI), ECCV Workshop 2008</i> (Oral Presentation) <br />
[<a href="https://uofi.box.com/shared/static/dkg455jk1gdd0vog2f8wuk7mjesg17r7.pdf">Paper (PDF)</a>]
[<a href="https://sites.google.com/site/jbhuang0604/publications/cvd_cvavi_2008">Project page</a>] <br /></p>
<p><b><a href="">CVGIP 2008 Best Paper Honorable Mention</a></b></p>
</td></tr></table>
<table class="imgtable"><tr><td>
<img src="images/projects/SPL 2007 Recoloring.jpg" alt="" width="400px" />&nbsp;</td>
<td align="left"><p><b>Information Preserving Color Transformation for Protanopia and Deuteranopia</b> <br />
<a href="http://www.jiabinhuang.com">Jia-Bin Huang</a>, Yu-Cheng-Tseng, Se-In Wu, and <a href="http://vlab.ee.nctu.edu.tw/advisor/">Sheng-Jyh Wang</a> <br />
<i>IEEE Signal Processing Letters, Vol. No.10, October 2007</i> <br />
[<a href="https://uofi.box.com/shared/static/uws1gh0busb0mlrmhjbtqr577p4mr3kp.pdf">Paper (PDF)</a>]
[<a href="https://sites.google.com/site/jbhuang0604/publications/cvd_spl_2007">Project page</a>] <br /></p>
<p><b><a href="">CVGIP 2006 Best Paper Honorable Mention</a></b></p>
</td></tr></table>
<p>
<a name="open-office-hour"></a>
</p>
<h2>Open Office Hour</h2>
<p>I set aside some time each week to meet with anyone (preferrably students from underrepresented groups).
Free free to sign up below if you would like to chat with me on any topics.</p>
<!-- Calendly inline widget begin -->
<div class="calendly-inline-widget" data-url="https://calendly.com/jbhuang0604/open-office-hour" style="min-width:320px;height:630px;"></div>
<script type="text/javascript" src="https://assets.calendly.com/assets/external/widget.js" async></script>
<!-- Calendly inline widget end -->
<a href="https://SiteStates.com" title="site stats">
    <img src="https://SiteStates.com/show/image/33170.jpg" border="0" />
</a>
</div>
</body>
</html>
